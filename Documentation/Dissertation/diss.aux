\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\bibstyle{plain}
\providecommand\@newglossary[4]{}
\@newglossary{main}{glg}{gls}{glo}
\providecommand\@glsorder[1]{}
\providecommand\@istfilename[1]{}
\@istfilename{diss.ist}
\@glsorder{word}
\citation{gamal2005cmos}
\citation{gamal2005cmos}
\citation{gamal2005cmos}
\citation{criminisi2013decision}
\citation{Qingli:2013:spectralImagingTech}
\citation{Bioucas-Dias:2012:unmixingOverview}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Terminology and preliminary definitions}{1}{section.1.1}}
\newlabel{sec:term_and_def}{{1.1}{1}{Terminology and preliminary definitions}{section.1.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces Examples of (a) a time-domain signal $f(t)$, (b) it's Fourier transform $F(\omega )$, (c) it's power spectrum $|F(\omega )|^2$ and (d) a quantisation of c.\relax }}{2}{figure.caption.11}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:spectrum}{{1.1}{2}{Examples of (a) a time-domain signal $f(t)$, (b) it's Fourier transform $F(\omega )$, (c) it's power spectrum $|F(\omega )|^2$ and (d) a quantisation of c.\relax }{figure.caption.11}{}}
\citation{Luthman:2015:hyperspectralImager}
\citation{Bioucas-Dias:2012:unmixingOverview}
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces Examples of a monochrome image (left) and a spectral image (right).\relax }}{3}{figure.caption.12}}
\citation{Luthman:2015:hyperspectralImager}
\citation{Luthman:2015:hyperspectralImager}
\citation{keshava2003survey}
\citation{Luthman:2015:hyperspectralImager}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Aims of the project}{4}{section.1.2}}
\newlabel{sec:what_are_we_trying_to_do}{{1.2}{4}{Aims of the project}{section.1.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Possible solutions}{4}{section.1.3}}
\citation{pham2000current}
\citation{criminisi2013decision}
\citation{zhao2016segmenting}
\citation{zhao2016multiscale}
\citation{conjeti2016supervised}
\citation{criminisi2012decision}
\citation{badrinarayanan2015segnet2}
\citation{jia2014caffe}
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Related work}{5}{section.1.4}}
\citation{klein2004lagrange}
\citation{law2006simple}
\citation{seymore1999learning}
\citation{cunningham2007k}
\citation{criminisi2013decision}
\citation{heaton2008introduction}
\citation{JMLR:v16:heaton15a}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Preparation}{7}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}An introduction to image segmentation}{7}{section.2.1}}
\citation{PAL19931277}
\citation{PAL19931277}
\citation{PAL19931277}
\citation{russell1995modern}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Supervised learning for classification}{8}{section.2.2}}
\newlabel{sec:supervised_learning}{{2.2}{8}{Supervised learning for classification}{section.2.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Decision trees and random forests}{9}{section.2.3}}
\newlabel{sec:rand_forest}{{2.3}{9}{Decision trees and random forests}{section.2.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Introduction to decision trees}{9}{subsection.2.3.1}}
\newlabel{sec:intro_decision_trees}{{2.3.1}{9}{Introduction to decision trees}{subsection.2.3.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Example of (part of) a tree used to classify humans.\relax }}{10}{figure.caption.13}}
\newlabel{fig:intuitive_tree}{{2.1}{10}{Example of (part of) a tree used to classify humans.\relax }{figure.caption.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Example of node numberings in a decision tree.\relax }}{10}{figure.caption.14}}
\citation{criminisi2013decision}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Example of how some instance $\mathbf  {x}$ would be classified using a decision tree.\relax }}{11}{figure.caption.15}}
\newlabel{fig:decision_tree_classify}{{2.3}{11}{Example of how some instance $\vc {x}$ would be classified using a decision tree.\relax }{figure.caption.15}{}}
\citation{criminisi2013decision}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Training a decision tree}{12}{subsection.2.3.2}}
\newlabel{eq:training_seq_entropy}{{2.7}{12}{Training a decision tree}{equation.2.3.7}{}}
\newlabel{eq:empirical_distribution}{{2.8}{12}{Training a decision tree}{equation.2.3.8}{}}
\newlabel{eq:left_split}{{2.9}{12}{Training a decision tree}{equation.2.3.9}{}}
\newlabel{eq:right_split}{{2.10}{12}{Training a decision tree}{equation.2.3.10}{}}
\newlabel{eq:information_gain}{{2.11}{12}{Training a decision tree}{equation.2.3.11}{}}
\citation{criminisi2013decision}
\citation{criminisi2013decision}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}Moving swiftly on to random forests}{13}{subsection.2.3.3}}
\newlabel{sec:moving_swiftly_onto_forests}{{2.3.3}{13}{Moving swiftly on to random forests}{subsection.2.3.3}{}}
\citation{gamal2005cmos}
\citation{gamal2005cmos}
\citation{gamal2005cmos}
\citation{gamal2005cmos}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Neural Networks}{14}{section.2.4}}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Imaging and image noise}{14}{section.2.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.1}Image sensor arrays}{14}{subsection.2.5.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces CCD and CMOS sensor arrays. Reproduced from Gamel et al \cite  {gamal2005cmos}.\relax }}{14}{figure.caption.16}}
\newlabel{fig:image_pipeline}{{2.4}{14}{CCD and CMOS sensor arrays. Reproduced from Gamel et al \cite {gamal2005cmos}.\relax }{figure.caption.16}{}}
\citation{gamal2005cmos}
\citation{gamal2005cmos}
\citation{gamal2005cmos}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Overview of an imaging `pipeline'. Reproduced from Gamel et al \cite  {gamal2005cmos}.\relax }}{15}{figure.caption.17}}
\newlabel{fig:image_pipeline}{{2.5}{15}{Overview of an imaging `pipeline'. Reproduced from Gamel et al \cite {gamal2005cmos}.\relax }{figure.caption.17}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces Parallel stream of photons incident on one sensor in a sensor array.\relax }}{15}{figure.caption.18}}
\newlabel{fig:photon_stream}{{2.6}{15}{Parallel stream of photons incident on one sensor in a sensor array.\relax }{figure.caption.18}{}}
\citation{hasinoff2014photon}
\citation{ross2002probability}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces The charge held on a capacitor is linear with respect time, until it hits some maximum \cite  {gamal2005cmos}.\relax }}{16}{figure.caption.19}}
\newlabel{fig:linear_charge_wrt_photon_rate}{{2.7}{16}{The charge held on a capacitor is linear with respect time, until it hits some maximum \cite {gamal2005cmos}.\relax }{figure.caption.19}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.2}Quantum noise}{16}{subsection.2.5.2}}
\newlabel{sec:quantum_noise}{{2.5.2}{16}{Quantum noise}{subsection.2.5.2}{}}
\newlabel{eq:image_poisson}{{2.20}{16}{Quantum noise}{equation.2.5.20}{}}
\citation{picano2004sustainability}
\citation{sprawls1987physical}
\citation{coupe2009nonlocal}
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces Example of Poisson($\mu $) and SPoisson($\mu $, 30) distributions, for $\mu = 5,15,25$.\relax }}{18}{figure.caption.20}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.3}Additive White Gaussian Noise}{18}{subsection.2.5.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.4}Medical imaging noise}{18}{subsection.2.5.4}}
\citation{figueiredo2006total}
\citation{rodrigues2008denoising}
\@writefile{toc}{\contentsline {section}{\numberline {2.6}TVMM Image De-Noising}{19}{section.2.6}}
\@writefile{toc}{\contentsline {section}{\numberline {2.7}Requirements analysis}{19}{section.2.7}}
\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces High level goals and desired outcomes for the project.\relax }}{20}{table.caption.21}}
\@writefile{toc}{\contentsline {section}{\numberline {2.8}System Design}{20}{section.2.8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.8.1}Pipeline/Overview}{20}{subsection.2.8.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces Overview of the system and its intended use.\relax }}{21}{figure.caption.22}}
\newlabel{fig:system_overview}{{2.9}{21}{Overview of the system and its intended use.\relax }{figure.caption.22}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.8.2}Interface/Usage}{21}{subsection.2.8.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.8.2.1}Training Tools}{21}{subsubsection.2.8.2.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.10}{\ignorespaces Overview of the training tools function.\relax }}{22}{figure.caption.23}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.8.2.2}Train}{22}{subsubsection.2.8.2.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.11}{\ignorespaces Overview of the train function.\relax }}{22}{figure.caption.24}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.8.2.3}Learn From Example}{22}{subsubsection.2.8.2.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.12}{\ignorespaces Overview of the \textit  {learn from example} function.\relax }}{23}{figure.caption.25}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.8.2.4}Pixel Labeller}{23}{subsubsection.2.8.2.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.13}{\ignorespaces Overview of the pixel labeller function.\relax }}{23}{figure.caption.26}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.8.2.5}De-Noiser}{24}{subsubsection.2.8.2.5}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.14}{\ignorespaces Overview of the de-noiser function.\relax }}{24}{figure.caption.27}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.8.2.6}Noisy Pixel Labeller}{24}{subsubsection.2.8.2.6}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.15}{\ignorespaces Overview of the noisy pixel labeller function.\relax }}{24}{figure.caption.28}}
\citation{JMLR:v16:heaton15a}
\@writefile{toc}{\contentsline {section}{\numberline {2.9}Languages and tools}{25}{section.2.9}}
\@writefile{toc}{\contentsline {section}{\numberline {2.10}Software engineering techniques}{26}{section.2.10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.10.1}Development model}{26}{subsection.2.10.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.10.2}Testing}{26}{subsection.2.10.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.10.3}Backup Plan}{27}{subsection.2.10.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.16}{\ignorespaces Overview of the backup strategy employed.}}{28}{figure.caption.29}}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Implementation}{29}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces The dependencies between different packages/modules in the system. Although the TrainingTools package is capable of producing training sequence files for both the NeuralNetworks and RandomDecisionForests, it only \textit  {depends} on RandomDecisionForests as it uses the class \texttt  {TrainingSequence}.\relax }}{30}{figure.caption.30}}
\newlabel{fig:package_dependencies}{{3.1}{30}{The dependencies between different packages/modules in the system. Although the TrainingTools package is capable of producing training sequence files for both the NeuralNetworks and RandomDecisionForests, it only \textit {depends} on RandomDecisionForests as it uses the class \texttt {TrainingSequence}.\relax }{figure.caption.30}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Random Forests Library}{30}{section.3.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces An overview of classes in the RandomDecisionForest package, from which relevant sections will be looked at closer when appropriate.\relax }}{31}{figure.caption.31}}
\newlabel{fig:forest_uml}{{3.2}{31}{An overview of classes in the RandomDecisionForest package, from which relevant sections will be looked at closer when appropriate.\relax }{figure.caption.31}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Data structures}{32}{subsection.3.1.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.1.1}ClassLabels}{32}{subsubsection.3.1.1.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces A closer view of \texttt  {ClassLabel} in Figure \ref  {fig:forest_uml}.\relax }}{32}{figure.caption.32}}
\newlabel{fig:class_label_uml}{{3.3}{32}{A closer view of \texttt {ClassLabel} in Figure \ref {fig:forest_uml}.\relax }{figure.caption.32}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces Important methods implemented in the \texttt  {ClassLabel} class.\relax }}{33}{table.caption.33}}
\newlabel{tab:ClassLabel}{{3.1}{33}{Important methods implemented in the \texttt {ClassLabel} class.\relax }{table.caption.33}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.1.2}Instances}{33}{subsubsection.3.1.1.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces A closer view of \texttt  {Instance} in Figure \ref  {fig:forest_uml}.\relax }}{34}{figure.caption.34}}
\newlabel{fig:instance_uml}{{3.4}{34}{A closer view of \texttt {Instance} in Figure \ref {fig:forest_uml}.\relax }{figure.caption.34}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.2}{\ignorespaces Important methods implemented in the \texttt  {NDRealVector} class.\relax }}{35}{table.caption.35}}
\newlabel{tab:NDRealVector}{{3.2}{35}{Important methods implemented in the \texttt {NDRealVector} class.\relax }{table.caption.35}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.1.3}Probability Distributions}{35}{subsubsection.3.1.1.3}}
\newlabel{sec:prob_dist}{{3.1.1.3}{35}{Probability Distributions}{subsubsection.3.1.1.3}{}}
\newlabel{eq:prob_entropy}{{3.1}{35}{Probability Distributions}{equation.3.1.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces A closer view of \texttt  {ProbabilityDistribution} in Figure \ref  {fig:forest_uml}.\relax }}{36}{figure.caption.36}}
\newlabel{fig:prob_dist_uml}{{3.5}{36}{A closer view of \texttt {ProbabilityDistribution} in Figure \ref {fig:forest_uml}.\relax }{figure.caption.36}{}}
\citation{1985--ieee754}
\newlabel{eq:sum}{{3.10}{37}{Probability Distributions}{equation.3.1.10}{}}
\newlabel{eq:numstabsum}{{3.11}{37}{Probability Distributions}{equation.3.1.11}{}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {3.1}Part of the \texttt  {ProbabilityDirstribution} constructor, where we set $\epsilon = 2^{-10}$.}{38}{lstlisting.3.1}}
\@writefile{lot}{\contentsline {table}{\numberline {3.3}{\ignorespaces justification=justified,singlelinecheck=false}}{39}{table.caption.38}}
\newlabel{tab:ProbabilityDistribution}{{3.3}{39}{justification=justified,singlelinecheck=false}{table.caption.38}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.1.4}Training Sequences}{39}{subsubsection.3.1.1.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces A closer view of \texttt  {TrainingSequence} and \texttt  {TrainingSample} in Figure \ref  {fig:forest_uml}.\relax }}{40}{figure.caption.39}}
\newlabel{fig:training_seq_uml}{{3.6}{40}{A closer view of \texttt {TrainingSequence} and \texttt {TrainingSample} in Figure \ref {fig:forest_uml}.\relax }{figure.caption.39}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.4}{\ignorespaces Important methods implemented in the \texttt  {TrainingSequence} class.\relax }}{41}{table.caption.40}}
\newlabel{tab:TrainingSequence}{{3.4}{41}{Important methods implemented in the \texttt {TrainingSequence} class.\relax }{table.caption.40}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.1.5}Split Parameters}{42}{subsubsection.3.1.1.5}}
\newlabel{sec:split_params}{{3.1.1.5}{42}{Split Parameters}{subsubsection.3.1.1.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces A closer view of \texttt  {SplitParamter} and \texttt  {OneDimensionalLinearSplitParameter} in figure \ref  {fig:forest_uml}.\relax }}{43}{figure.caption.41}}
\newlabel{fig:split_param_uml}{{3.7}{43}{A closer view of \texttt {SplitParamter} and \texttt {OneDimensionalLinearSplitParameter} in figure \ref {fig:forest_uml}.\relax }{figure.caption.41}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.1.6}Weak Learners}{43}{subsubsection.3.1.1.6}}
\newlabel{sec:weak_learner}{{3.1.1.6}{43}{Weak Learners}{subsubsection.3.1.1.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.8}{\ignorespaces A closer view of the \texttt  {WeakLearnerType} enum in Figure \ref  {fig:forest_uml}.\relax }}{44}{figure.caption.42}}
\newlabel{fig:weak_learner_type_uml}{{3.8}{44}{A closer view of the \texttt {WeakLearnerType} enum in Figure \ref {fig:forest_uml}.\relax }{figure.caption.42}{}}
\newlabel{eq:split_equation}{{3.12}{44}{Weak Learners}{equation.3.1.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.9}{\ignorespaces justification=justified,singlelinecheck=false}}{45}{figure.caption.43}}
\newlabel{fig:weak_learner_uml}{{3.9}{45}{justification=justified,singlelinecheck=false}{figure.caption.43}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.5}{\ignorespaces Important methods implemented in the \texttt  {OneDimensionalLinearWeakLearner} class.\relax }}{46}{table.caption.44}}
\newlabel{tab:OneDimensionalLinearWeakLearner}{{3.5}{46}{Important methods implemented in the \texttt {OneDimensionalLinearWeakLearner} class.\relax }{table.caption.44}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.10}{\ignorespaces Knowing the minimum and maximum values in each dimension allows us to make an informed choice on split parameters to pick, which in this case is the green zone of values.\relax }}{47}{figure.caption.45}}
\newlabel{fig:split_parameter_choice_optimisation}{{3.10}{47}{Knowing the minimum and maximum values in each dimension allows us to make an informed choice on split parameters to pick, which in this case is the green zone of values.\relax }{figure.caption.45}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.1.7}Decision Tree Node}{47}{subsubsection.3.1.1.7}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.11}{\ignorespaces A closer view of \texttt  {TreeNode} and \texttt  {DecisionForest} in Figure \ref  {fig:forest_uml}.\relax }}{48}{figure.caption.46}}
\newlabel{fig:weak_learner_uml}{{3.11}{48}{A closer view of \texttt {TreeNode} and \texttt {DecisionForest} in Figure \ref {fig:forest_uml}.\relax }{figure.caption.46}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.12}{\ignorespaces An example of how \texttt  {compact} could be used to reduce the size of a decision tree.\relax }}{49}{figure.caption.47}}
\@writefile{lot}{\contentsline {table}{\numberline {3.6}{\ignorespaces Important methods implemented in the \texttt  {TreeNode} class.\relax }}{49}{table.caption.49}}
\newlabel{tab:TreeNode}{{3.6}{49}{Important methods implemented in the \texttt {TreeNode} class.\relax }{table.caption.49}{}}
\newlabel{lst:TreeNode}{{3.2}{50}{The \texttt {TreeNode} declaration, found as a static class within the \texttt {DecisionForest} class}{lstlisting.3.2}{}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {3.2}The \texttt  {TreeNode} declaration, found as a static class within the \texttt  {DecisionForest} class.}{50}{lstlisting.3.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.1.8}Decision Forest}{51}{subsubsection.3.1.1.8}}
\newlabel{sec:DecisionForest}{{3.1.1.8}{51}{Decision Forest}{subsubsection.3.1.1.8}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.7}{\ignorespaces Important methods implemented in the \texttt  {DecisionForest} class.\relax }}{51}{table.caption.50}}
\newlabel{tab:DecisionForest}{{3.7}{51}{Important methods implemented in the \texttt {DecisionForest} class.\relax }{table.caption.50}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}Training}{51}{subsection.3.1.2}}
\newlabel{sec:training}{{3.1.2}{51}{Training}{subsection.3.1.2}{}}
\newlabel{lst:generateTree}{{3.3}{52}{Psuedocode to train a decision tree}{lstlisting.3.3}{}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {3.3}Psuedocode to train a decision tree.}{52}{lstlisting.3.3}}
\citation{criminisi2013decision}
\citation{criminisi2013decision}
\@writefile{lof}{\contentsline {figure}{\numberline {3.13}{\ignorespaces Visualisation of breadth first vs depth first. Optimisation of information gain takes place over all of the nodes in the frontier. Recreated from Criminisi et al \cite  {criminisi2013decision}.\relax }}{54}{figure.caption.52}}
\newlabel{fig:breadth_first}{{3.13}{54}{Visualisation of breadth first vs depth first. Optimisation of information gain takes place over all of the nodes in the frontier. Recreated from Criminisi et al \cite {criminisi2013decision}.\relax }{figure.caption.52}{}}
\newlabel{lst:train}{{3.4}{54}{Psuedocode to train a decision forest}{lstlisting.3.4}{}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {3.4}Psuedocode to train a decision forest.}{54}{lstlisting.3.4}}
\newlabel{lst:traverseTree}{{3.5}{55}{Psuedocode for traversing a decision tree}{lstlisting.3.5}{}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {3.5}Psuedocode for traversing a decision tree.}{55}{lstlisting.3.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.3}Classification}{55}{subsection.3.1.3}}
\newlabel{sec:classification}{{3.1.3}{55}{Classification}{subsection.3.1.3}{}}
\newlabel{lst:classify}{{3.6}{56}{Psuedocode for classification using a decision tree}{lstlisting.3.6}{}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {3.6}Psuedocode for classification using a decision tree.}{56}{lstlisting.3.6}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Neural Networks}{56}{section.3.2}}
\newlabel{sec:nn_training}{{3.2}{56}{Neural Networks}{section.3.2}{}}
\newlabel{sec:pixel_label}{{3.2}{56}{Neural Networks}{section.3.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Pixel Labelling}{56}{section.3.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.14}{\ignorespaces The UML diagram for the training tools, consisting of a single class with a single static method.\relax }}{57}{figure.caption.56}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Training Tools}{58}{section.3.4}}
\newlabel{sec:training_tools}{{3.4}{58}{Training Tools}{section.3.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.15}{\ignorespaces The UML diagram for the training tools, consisting of a single class with a single static method.\relax }}{58}{figure.caption.57}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.16}{\ignorespaces An example input to the training tools module: a ground truth pixel labelling, a spectral image and a class to colour mapping file (as defined in appendix \ref  {app:file_formats}).\relax }}{59}{figure.caption.58}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.17}{\ignorespaces Example of part of the output file (2 training samples) from the training tools module. (The actual file is over 88000 lines long).\relax }}{60}{figure.caption.59}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.18}{\ignorespaces An example of a pixel labelling we might want to provide, when we wish to ignore large regions of the image.\relax }}{60}{figure.caption.60}}
\newlabel{fig:non_perfect_labelling}{{3.18}{60}{An example of a pixel labelling we might want to provide, when we wish to ignore large regions of the image.\relax }{figure.caption.60}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}De-noising}{60}{section.3.5}}
\newlabel{sec:de-noising}{{3.5}{60}{De-noising}{section.3.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.6}Application on example data sets}{61}{section.3.6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.1}Siri's data}{61}{subsection.3.6.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.2}Teng's data}{61}{subsection.3.6.2}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Evaluation}{63}{chapter.4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Performance measures for classifiers}{63}{section.4.1}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Evaluation of the Random Forests library}{63}{section.4.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Training time}{64}{subsection.4.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Classification time}{64}{subsection.4.2.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.3}The effect of the number of trees}{64}{subsection.4.2.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.4}The effect of the depth of trees}{64}{subsection.4.2.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.5}The effect of the randomness of trees}{64}{subsection.4.2.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.6}The effect of normalisation}{64}{subsection.4.2.6}}
\newlabel{sec:effect_of_normalisation}{{4.2.6}{64}{The effect of normalisation}{subsection.4.2.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}The effect of the de-noising component}{64}{section.4.3}}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Conclusion}{65}{chapter.5}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Summary}{65}{section.5.1}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Further Work}{65}{section.5.2}}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Lessons Learned}{65}{section.5.3}}
\bibdata{refs}
\bibcite{1985--ieee754}{1}
\bibcite{badrinarayanan2015segnet2}{2}
\bibcite{Bioucas-Dias:2012:unmixingOverview}{3}
\bibcite{conjeti2016supervised}{4}
\bibcite{coupe2009nonlocal}{5}
\bibcite{criminisi2013decision}{6}
\bibcite{criminisi2012decision}{7}
\bibcite{cunningham2007k}{8}
\bibcite{figueiredo2006total}{9}
\@writefile{toc}{\contentsline {chapter}{Bibliography}{67}{section.5.3}}
\bibcite{gamal2005cmos}{10}
\bibcite{hasinoff2014photon}{11}
\bibcite{heaton2008introduction}{12}
\bibcite{JMLR:v16:heaton15a}{13}
\bibcite{jia2014caffe}{14}
\bibcite{keshava2003survey}{15}
\bibcite{klein2004lagrange}{16}
\bibcite{law2006simple}{17}
\bibcite{Qingli:2013:spectralImagingTech}{18}
\bibcite{Luthman:2015:hyperspectralImager}{19}
\bibcite{PAL19931277}{20}
\bibcite{pham2000current}{21}
\bibcite{picano2004sustainability}{22}
\bibcite{rodrigues2008denoising}{23}
\bibcite{ross2002probability}{24}
\bibcite{russell1995modern}{25}
\bibcite{seymore1999learning}{26}
\bibcite{sprawls1987physical}{27}
\bibcite{zhao2016segmenting}{28}
\bibcite{zhao2016multiscale}{29}
\@writefile{toc}{\contentsline {chapter}{\numberline {A}File formats}{71}{appendix.A}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{app:file_formats}{{A}{71}{File formats}{appendix.A}{}}
\@writefile{toc}{\contentsline {section}{\numberline {A.1}(Example/Noisy) Spectral Image}{71}{section.A.1}}
\@writefile{toc}{\contentsline {section}{\numberline {A.2}Example Image Labelling}{71}{section.A.2}}
\@writefile{toc}{\contentsline {section}{\numberline {A.3}Label Map}{71}{section.A.3}}
\@writefile{toc}{\contentsline {section}{\numberline {A.4}Training Sequence}{71}{section.A.4}}
\@writefile{toc}{\contentsline {section}{\numberline {A.5}Output Files}{72}{section.A.5}}
\@writefile{toc}{\contentsline {chapter}{\numberline {B}Code for training a decision forest}{73}{appendix.B}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{app:train_tree}{{B}{73}{Code for training a decision forest}{appendix.B}{}}
\newlabel{lst:actualGenerateTree}{{B.1}{73}{The implementation code for tree generation}{lstlisting.B.1}{}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {B.1}The implementation code for tree generation.}{73}{lstlisting.B.1}}
\newlabel{lst:actualTrainForest}{{B.2}{75}{The implementation code for training a decision forest}{lstlisting.B.2}{}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {B.2}The implementation code for training a decision forest.}{75}{lstlisting.B.2}}
\@writefile{toc}{\contentsline {chapter}{\numberline {C}Code for classifying using a decision forest}{79}{appendix.C}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{app:classify_tree}{{C}{79}{Code for classifying using a decision forest}{appendix.C}{}}
\newlabel{lst:actualTraverseTree}{{C.1}{79}{The implementation code for tree traversal}{lstlisting.C.1}{}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {C.1}The implementation code for tree traversal.}{79}{lstlisting.C.1}}
\newlabel{lst:actualTraverseTree}{{C.2}{80}{The implementation code for classification using a decision forest}{lstlisting.C.2}{}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {C.2}The implementation code for classification using a decision forest.}{80}{lstlisting.C.2}}
\@writefile{toc}{\contentsline {chapter}{\numberline {D}Project Proposal}{83}{appendix.D}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\numberline {E}Glossary}{95}{appendix.E}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
