\documentclass[12pt,twoside,notitlepage]{report}

% Some texcount commands so that we count text that occurs in tables
%TC:group table 0 1
%TC:group tabular 1 1
%TC:group tabularx 2 1
%TC:group lstlisting 0 1

%packages
\usepackage{float}                  % Allow use of H for figures, to allow for exact placement
\usepackage{a4}
\usepackage{hyperref}
\usepackage{verbatim}
\usepackage{booktabs, longtable}
\usepackage{tabularx}
\usepackage{xcolor,colortbl}
\usepackage{hhline}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{epstopdf}
\usepackage{listings}       % Source code listings
\usepackage[toc]{glossaries}
\usepackage{changebar}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{rotating}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{pgfplotstable}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{framed}                 % Frames/boxes - used in appendix describing file formats
\usepackage{enumitem}               % Fancier item lists - used for lists in languages and tools section
\usepackage{hyperref}               % Reference urls
\usepackage{tabularx}               % Allows the width of a tabular environment to be specified along with the use of X in the collumn specifiers
\usepackage{mathtools}
\usepackage[bottom]{footmisc}

% Shorthands
% Var in mathmode
\newcommand{\var}{\text{Var}}
% vectors in mathmode
\newcommand{\vc}[1]{\mathbf{#1}}
% math fonts shorthand
\newcommand{\cl}[1]{\mathcal{#1}}
\newcommand{\bb}[1]{\mathbb{#1}}
%Argmin/max operators
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
% Maximum likelihood and maximum a-posterior
\newcommand{\ML}{\text{ML}}
\newcommand{\MAP}{\text{MAP}}

% Names for the datasets we have
\newcommand{\siris}{[[Siri's]]}
\newcommand{\tengs}{[[Teng's]]}

% Def eq math symbol
\newcommand\defeq{\stackrel{\mathclap{\normalfont\tiny\mbox{def}}}{=}}

% Numbering on subsubsections
\setcounter{secnumdepth}{3}

% Tikz imports
\usetikzlibrary{
    arrows,
    shapes,
    snakes,
    automata,
    backgrounds,
    calc,
    petri,
    patterns,
}

% Tikz styles
\pgfplotsset{soldot/.style={color=blue,only marks,mark=*}} 
\pgfplotsset{holdot/.style={color=blue,fill=white,only marks,mark=*}}

%Setup listings env for when we don't want to use minted
\lstset{
  basicstyle=\footnotesize\tt,                                          % the size of the fonts that are used for the code
  breakatwhitespace=false,                                              % sets if automatic breaks should only happen at whitespace
  breaklines=true,                                                      % sets automatic line breaking
  prebreak = \raisebox{0ex}[0ex][0ex]{\ensuremath{\hookleftarrow}},     % add arrows to indicate line breaks added
  framesep = 5px,                                                       % add padding to the top/bottom of listings
  captionpos=b,                                                          % sets the caption-position to bottom
  extendedchars=true,                                                   % lets you use non-ASCII characters; for 8-bits encodings only, does not work with UTF-8
  frame=single,                                                             % adds a frame around the code (frame at bot and top)
  language=Java,                                                        % the language of the code
  showspaces=false,                                                     % show spaces everywhere adding particular underscores; it overrides 'showstringspaces'
  showstringspaces=false,                                               % underline spaces within strings only
  showtabs=false,                                                       % show tabs within strings adding particular underscores
  tabsize=4,                                                            % sets default tabsize to 4 spaces
  numbers=left,                                                         % puts numbers on the left side
  keywordstyle=\color[rgb]{0,0,1}\ttfamily,
  stringstyle=\color[rgb]{0.627,0.126,0.941}\ttfamily,
  commentstyle=\color[rgb]{0.133,0.545,0.133}\ttfamily,
  morecomment=[l][\color{magenta}]{\#},
  aboveskip=2em,                                                        % spacing above listings
  belowskip=1.5em                                                         % spacing below listings
}

% Graphics extensions used
\DeclareGraphicsExtensions{.pdf, .png, .jpg, .eps}
\graphicspath{ {figs/} }

% ??
\newcolumntype{R}{>{\raggedleft\arraybackslash}X}

% Pick the bibliocraphy style
\bibliographystyle{plain}

% Styling for pgf plots
\pgfplotsset{compat=newest}
\pgfplotsset{grid style={gray!50}}
\pgfplotsset{minor grid style={dashed,gray!50}}

%colours for bar graphs
\pgfplotsset{
   /pgfplots/bar  cycle  list/.style={/pgfplots/cycle  list={%
        {blue,fill=blue!50!white,mark=none},%
        {orange,fill=orange!50!white,mark=none},%
        {green,fill=green!50!white,mark=none},%
        {yellow,fill=yellow!40!white,mark=none},%
        {red,fill=red!50!white,mark=none},%
     }
   },
}

\usepgfplotslibrary{units}

% to allow postscript inclusions
\input{epsf}                 
% On thor and CUS read top of file:
%     /opt/TeX/lib/texmf/tex/dvips/epsf.sty
% On CL machines read:
%     /usr/lib/tex/macros/dvips/epsf.tex

% glossaries
\makeglossaries

\raggedbottom                           % try to avoid widows and orphans
\sloppy
\clubpenalty1000%
\widowpenalty1000%

\addtolength{\oddsidemargin}{6mm}       % adjust margins
\addtolength{\evensidemargin}{-8mm}

\renewcommand{\baselinestretch}{1.1}    % adjust line spacing to make
                                        % more readable

\begin{document}


% glossary definitions
% \newacronym{btb}{BTB} {Branch Target Buffer}
% \newacronym{bp0}{BTB8} {branch predictor with BTB and tablesize 8}
% \newacronym{bp1}{BTB512} {branch predictor with BTB and tablesize 512}
% \newacronym{sat}{SAT} {branch predictor with 3-bit saturating counters}
% \newacronym{lhbp}{LHBP} {local history branch predictor}
% \newacronym{pc}{PC} {Program Counter}
% \newacronym{alu}{ALU} {Arithmetic Logic Unit}
% \newacronym{risc}{RISC} {Reduced Instruction Set Computer}
% \newacronym{ISA}{ISA} {Instruction Set Architecture}
% \newacronym{ipc}{IPC} {Instructions Per Cycle}
% \newacronym{cpi}{CPI} {Cycles per Instruction}
% \newacronym{fpga}{FPGA} {Field Programmable Logic Array}
% \newacronym{alut}{ALUT}{Adaptive Look-Up Table}
% \newacronym{alm}{ALM} {Adaptive Logic Module}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Title

% Name
\begin{flushright}
    \Large
    \hfill{\LARGE \bf Michael Painter}
\end{flushright}

% Title + imgs
\begin{center}
    \vfill

    \Huge{\bf Spectral Image Analysis for Medical Imaging}
    
    \bigskip
    \bigskip
    
    {Part II Computer Science Tripos} \\
    
    \bigskip
    
    {Churchill College, 2016\\}

    \bigskip 

    {\today}

    \bigskip

    \includegraphics[scale=0.099]{titleimg/camcrest}
    \includegraphics[scale=0.2792]{titleimg/chucrest}

    \vfill
\end{center}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Proforma

\cleardoublepage

\setcounter{page}{1}
\pagenumbering{roman}
\pagestyle{plain}

\chapter*{Proforma}

{\large
\begin{tabularx}{\textwidth}{l X}
Name:               & \bf Michael Painter                      \\
College:            & \bf Churchill College                     \\
Project Title:      & \bf Spectral Image Analysis for Medical Imaging \\
Examination:        & \bf Computer Science Part II Project Dissertation, May 2016        \\
Word Count:         & \bf 8432\footnotemark[1]\\
Project Originator: & Dr Pietro Lio'              \\
Supervisor:         & Dr Pietro Lio' \& Dr Gianluca Ascolani       \\ 
\end{tabularx}
}

\footnotetext[1]{
    This word count was computed by {\tt texcount.pl -total diss.tex }
}
\stepcounter{footnote}


\section*{Original aims of the project}
    *TODO*

\section*{Work completed}
    *TODO*

\section*{Special difficulties}
    None.
 




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Declaration

\cleardoublepage
\section*{Declaration}

I, Michael Painter of Churchill College, being a candidate for Part II of the Computer Science Tripos, hereby declare 
that this dissertation and the work described in it are my own work, unaided except as may be specified below, and that 
the dissertation does not contain material that has already been used to any substantial extent for a comparable 
purpose.

\bigskip
\bigskip
\bigskip
\bigskip

\leftline{\rule{6cm}{0.5pt}}
\leftline{Signed}

\bigskip
\bigskip
\bigskip
\bigskip

\leftline{\rule{4cm}{0.5pt}}
\leftline{Date}










%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% the contents

\cleardoublepage
\setcounter{tocdepth}{1}
\tableofcontents










%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% list of figures, tables and listings

\listoffigures

\listoftables

\lstlistoflistings










%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% acknowledgements

\clearpage
\section*{Acknowledgements}

I thank my supervisors, director of studies and tutor for their extensive support, especially for putting up with me 
throughout the year. I would also like to thank Malavika Nair, Ioana Bica and Yanie de Nadaillac for 
all of their support and helping me keep it together through the tougher parts of the year.










%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% now for the chapters

\cleardoublepage        % just to make sure before the page numbering
                        % is changed

\setcounter{page}{1}
\pagenumbering{arabic}
\pagestyle{headings}















%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% the introduction

\cleardoublepage
\chapter{Introduction}
    In this chapter we introduce the problem: what sort of data are we given and what would we like to do with it? We 
    begin by introducing new terminology, and specifically defining terms that might otherwise be ambiguous. We briefly 
    discus the motivation behind the implementation of this system, where we then suggest some possible methods that 
    could be used to implement a solution, decide one that we might want to explore and what we might hope to discover 
    by exploring said solution. Finally we will outline the overall pipeline of the system and define an interface for 
    the system.

    \section{Terminology and preliminary definitions} \label{sec:term_and_def}
        To begin with we will introduce some terminology and preliminary definitions that we will use throughout the 
        dissertation, including the rest of the introduction chapter.

        Firstly we will use $\bb{N}_k$ where $k\in \bb{N}$ to denote the set $\{ n \in \bb{N}\ |\ n < k \} = 
        \{0, 1, ..., (k-1)\}$. We will also use $\bb{B}$ to denote the set $\{0, 1\} = \bb{N}_2$, for a boolean choice.

        Typically we use the word \textit{spectral} to refer to use of the Fourier domain, for example 
        \textit{spectral methods} refers to the use of Fourier domain to solve differential equations. For our purposes 
        we will say that the \textit{spectrum} of some signal is the Fourier transform of the time-domain signal, more 
        specifically, we will usually be referring to a quantised spectrum. We will use the word \textit{spectrum} to 
        refer to either a continuous or quantised spectrum, for which the context should make it obvious which one is 
        intended. 

        \begin{figure}[H]
            \centering

                \scalebox{0.80}{ 
                    \begin{tikzpicture}
                        \begin{scope}
                            \begin{axis}[
                                name = plot1,
                                axis lines = left,
                                xlabel = $t$,
                                ylabel = $f(t)$
                            ]
                                \addplot[domain=-3:0, blue] {0};
                                \addplot[domain=0:1, blue] {1};
                                \addplot[domain=1:3, blue] {0};
                                \draw[dotted] (axis cs:0,0) -- (axis cs:0,1);
                                \draw[dotted] (axis cs:1,1) -- (axis cs:1,0);
                                \addplot[holdot] coordinates{(0,0)(0,1)(1,1)(1,0)};
                                \addplot[soldot] coordinates{(0,0.5)(1,0.5)};
                            \end{axis}
                            \node at (plot1.above north west) [anchor=south west] {(a)};
                        \end{scope}

                        \begin{scope}[xshift=9cm]
                            \begin{axis}[
                                name = plot2,
                                axis lines = left,
                                xlabel = $\omega$,
                                ylabel = $F(\omega)$
                            ]
                                \addplot[domain=-3:3, samples = 500, red] {((sin(deg(x) * pi))/((deg(x) * pi) * 1.74 * 0.01)) * sin(-(deg(x)*pi)/2)};
                                \addlegendentry{$\Re(F(\omega))$}
                                \addplot[domain=-3:3, samples = 500, blue] {((sin(deg(x) * pi))/((deg(x) * pi) * 1.74 * 0.01)) * cos(-(deg(x)*pi)/2)};
                                \addlegendentry{$\Im(F(\omega))$}
                                \draw (axis cs:-3,0) -- (axis cs:3,0);
                            \end{axis}
                            \node at (plot2.above north west) [anchor=south west] {(b)};
                        \end{scope}

                        \begin{scope}[yshift=-8cm]
                            \begin{axis}[
                                name = plot3,
                                axis lines = left,
                                xlabel = $\omega$,
                                ylabel = $\left|F(\omega)\right|^2$
                            ]
                                \addplot[domain=-3:3, samples = 500, blue] {((sin(deg(x) * pi))/((deg(x) * pi) * 1.74 * 0.01))^2};
                            \end{axis}
                            \node at (plot3.above north west) [anchor=south west] {(c)};
                        \end{scope}

                        \begin{scope}[xshift=9cm, yshift=-8cm]
                            \begin{axis}[
                                name = plot4,
                                axis lines = left,
                                xlabel = $\omega$,
                            ]
                                \addplot[ybar interval, draw=blue,pattern=horizontal lines light blue] coordinates {
                                    (-3,0.02) 
                                    (-2,0.045)
                                    (-1,0.1)
                                    (-0.5,0.7)
                                    (-0.125,1)
                                    (0.125,0.7)
                                    (0.5,0.1)
                                    (1,0.045)
                                    (2,0.02)
                                    (3,0.02)
                                };
                                \addplot[domain=-3:3, samples = 100, dashed] {((sin(deg(x) * pi)/((deg(x) * pi) * 1.74 * 0.01))^2};
                            \end{axis}
                            \node at (plot4.above north west) [anchor=south west] {(d)};
                        \end{scope}


                    \end{tikzpicture}
                }


            \caption{Examples of (a) a time-domain signal $f(t)$, (b) it's Fourier transform $F(\omega)$, (c) it's power 
                spectrum $|F(\omega)|^2$ and (d) a quantisation of c.}
            \label{fig:spectrum}
        \end{figure}

        We will use the term \textit{spectral bin} to refer to a interval of wavelengths. Typically we will be referring 
        to the case where we are dealing with quantised spectra, where the spectral bin refers to the range of 
        wavelengths some bar in the histogram represents. For example consider (c) in figure \ref{fig:spectrum}.

        We will use \textit{spectral image} to refer to an image with an arbitrary number of frequency components. More 
        formally we define an \textit{image} as a function $I : \bb{N}_w \times \bb{N}_h \rightarrow \bb{N}$, 
        where $w$ is the width, $h$ is the height. An image 
        is simply a function of two spatial values. For a \textit{spectral image} $I'$ we extend the concept to a 
        function $I' : \bb{N}_w \times \bb{N}_h \times \bb{N}_f \rightarrow \bb{N}$, where $w$ is the width, 
        $h$ is the height and $f$ is the number of spectral bins. We call this a spectral image, because at each `pixel' 
        (coordinate), we have a spectrum of values, that is we can define the spectrum $s_{xy}$ of the pixel $(x,y)$ by
        $s_{xy}(\cdot) = I'(x,y,\cdot)$. An intuitive way to think of a \textit{spectral image} is a cube of values, so 
        a spectral image is commonly referred to as a \textit{data cube} (as in \cite{Qingli:2013:spectralImagingTech} 
        and \cite{Bioucas-Dias:2012:unmixingOverview}) and we will use this interchangeably with the term 
        \textit{spectral image}.

        \begin{figure}[H]
            \centering
            \includegraphics[scale=1.0]{spectral_image}
            \caption{Examples of a monochrome image (left) and a spectral image (right).}
        \end{figure}

        We specifically note that a common form of spectral image is an ``RGB'' image, where we have $f=3$, with a 
        spectral bin for the red, green and blue regions of the visible spectrum.

        Finally some literature refers to \textit{multispectral} and \textit{hyperspectral} images (which may be 
        referenced such as in \cite{Luthman:2015:hyperspectralImager} and \cite{Bioucas-Dias:2012:unmixingOverview}), 
        which are simply special cases of a spectral image. A multispectral image refers to a small number of spectral 
        bands (such as 5) and a hyperspectral image refers to a large number of spectral bands (such as 50). Intuitively 
        we can think of the difference between multispectral and hyperspectral in terms of their spectra, a 
        multispectral image should be thought of as having quantised spectra (histograms), whereas a hyperspectral 
        image can be thought of as attempting to approximate the continuous spectra.


    \section{What are we trying to do?} \label{sec:what_are_we_trying_to_do}
        Nowadays computer vision techniques are used in abundance to aid medical diagnosis, and are used by doctors 
        to help provide more accurate and earlier diagnoses. This is clearly an area of interest, as such tools 
        may help improve, or even save lives where we wouldn't have been able to do so in the past. 

        In this project we consider using spectral images (recall that this includes RGB images) for medical purposes, 
        and we wish to segment the image to indicate different regions of interest, typically indicating some form of 
        disease of abnormality. The problem can easily be described mathematically: given an arbitrary spectral image 
        $I : \bb{N}_w \times \bb{N}_h \times \bb{N}_f \rightarrow \bb{N}$ and a set of possible classes 
        $\cl{C}$, we want to output a \textit{pixel labelling} $\cl{P} : \bb{N}_w \times \bb{N}_h 
        \rightarrow \cl{C}$ that correctly identifies regions of the image according to $\cl{C}$. Succinctly 
        put, we want to perform \textit{image segmentation}, on (noisy) medical (spectral) images.

        The motivation for this project comes from use of a hyperspectral imager for use with contrast agents 
        \cite{Luthman:2015:hyperspectralImager}. The aim is that the imager can be used to identify the presence of 
        fluorescent contrast agents, some of which have a negative binding response to cancerous cells in the oesophagus, 
        so allow cancerous tissues to be identified.

        To summarise, we wish to produce a system capable of producing a pixel labelling from a spectral image. The 
        pixel labelling will depend highly on the data and so we would likely wish to learn from some examples.

    \section{Possible solutions}
        \begin{itemize}
            \item TODO: Describe datasets - Siri's, Tengs
        \end{itemize}

        As we will be looking to segment images, we will need to identify regions of the image based on the data 
        available. One approach that we might take is to try and identify some properties manually, for example one 
        possible solution in the \siris dataset is to explicitly find the spectra of \textit{endmembers} (the 
        fluorescents) in the images and use a \textit{spectral unmixing} algorithm \cite{keshava2003survey} such as 
        constrained least squares to identify the proportions of endmembers present at each given point 
        \cite{Luthman:2015:hyperspectralImager}.

        An alternative that we will pursue is utilising machine learning methods, where we might hope to learn 
        relationships within the data through supervised learning. Such a system may be more versatile, able to 
        solve many similar problems and not constrained to a specific problem. For example, the spectral unmixing 
        solution above requires the endmembers to be measurable, requiring significant work prior to putting the data 
        through the algorithm. We would hope by using machine learning that we can avoid such additional work, and even 
        solve problems where the `endmembers' aren't separable, having explicitly measurable spectra.

    \section{Related work}
        Medical imaging is an active research area and will likely be for a long time. Medical imaging is ubiquitous in 
        hospitals and an essential tool to aid doctors in diagnosing a patient. Along with medical imaging comes 
        medical image analysis, where image processing methods are used to give more information to doctors which they 
        may not be able to see with the naked eye.The task of image segmentation is important in medicine illnesses and 
        viruses are commonly localised, only affecting parts of tissues and identification of which parts of tissues 
        are healthy or not is important to successful treatment.

        There is plenty of work and research that has gone into image segmentation, and it is common technique used in 
        medical imaging, implemented using a variety of methods such as classifiers, thresholding and region growing
        \cite{pham2000current}. However non-machine learning methods seem to be out of trend, with it being 
        difficult to find papers written in this decade, whilst these methods are likely still being actively 
        researched, and almost definitely still used in hospitals, they are currently outweighed vastly by machine 
        learning papers. 

        Lately neural networks has seen a major dominance with regards to machine learning, however within medical 
        imaging other areas of machine learning are still finding success, with whole books still dedicated to the 
        topic \cite{criminisi2013decision}. True to the current trend, there is plenty of current work utilising 
        neural networks in medical imaging, for example they have been successfully applied to segment different 
        regions of the brain, such as `white matter', `gray matter', and `cerebrospinal fluid' \cite{zhao2016segmenting},
        and similarly for segmentation of brain tumours \cite{zhao2016multiscale}.

        However, other machine learning methods are still of interest and random forests are still an active area of 
        research within medical imaging and computer vision \cite{conjeti2016supervised}, especially because of their 
        simplicity and lack of complexity, potentially making them very efficient \cite{criminisi2012decision}.

        SegNet \cite{badrinarayanan2015segnet2} is a deep convolution neural network architecture built on top of Caffe 
        \cite{jia2014caffe} for pixel wise labelling, and provides some inspiration for the project. SegNet uses 
        supervised learning to train the neural network, takes some image as input and outputs a pixel labelling. An extension 
        to SegNet, Bayesian SegNet, includes an additional output image, which is a mapping of uncertainty in each pixel labelling, 
        something that we will want to include in our own system. The Caffe implementation includes support for OpenCL 
        (an open source GPU programming language), that allows for an efficient implementation of SegNet, realised by 
        applying it to run on a real-time video stream, with 360 by 480 resolution. Whilst this implementation isn't 
        necessarily geared towards medical purposes, it could certainly be utilised for a medical application, and 
        is a good example of a system solving the more general problem of image segmentation/pixel labelling.











%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% the preparation

\cleardoublepage
\chapter{Preparation}
    In this section we will introduce the area of image segmentation and how we may perform segmentation. Then we 
    introduce decision trees and explain the supervised learning method of random forests. Many medical imaging 
    techniques tend to require short exposure times they frequently exhibit visible noise, leading to grainy images. So, 
    we then discuss how we may want to model the noise and how we can deal with this. In the second half of the chapter 
    we outline the design with a requirements analysis, pipeline overview, what tools we will use and how we will 
    evaluate our performance.

    From a list of machine learning algorithms: State Vector Machine \cite{klein2004lagrange}, \cite{law2006simple}, 
    Hidden Markov Models \cite{seymore1999learning}, k Nearest Neighbour \cite{cunningham2007k}, Random Forests 
    \cite{criminisi2013decision}, and Neural Networks \cite{heaton2008introduction}. We decide that two algorithms that 
    would be useful in our case are Random Forests and Neural Networks, so we will implement a Random Forests library 
    and use the Encog library for Neural Networks in Java \cite{JMLR:v16:heaton15a}.



    \section{An introduction to image segmentation}
        Image segmentation concerns splitting an image into connected subsets, in some meaningful way. It is typically an 
        ill-posed problem with no `best' segmentation, however we often have some desired segmentation that we would 
        like our system to perform, which we usually refer to as `ground truth'. When training a supervised learning 
        system we provide some number of ground truth images. 

        More formally if we let $\Omega$ be the set of pixels in some image, a valid image segmentation is $(S_1, ..., 
        S_n)$ for some $n \in \bb{N}$ which satisfy the following:

        \begin{align}
            \Omega = \bigcup\limits_{i=1}^n S_i \\
            S_i \cap S_j = \emptyset && \text{for all } i \neq j 
        \end{align}

        and each $S_i$ is connected, which means that there is a path between any two pixels of $S_i$, 
        where a path may take steps of one pixel up, left, down or right \cite{PAL19931277}. 

        There are a number of ways that we may try to perform an image segmentation. One such method consists of 
        finding the edges present in an image, which can be found using the zero crossings of some gradient operator 
        such as the Laplacian. Furthermore, we may want to look at edges at different scales, which suggests blurring 
        the image before taking the gradient operator, leading to the Laplacian of Gaussian operator, as discussed by 
        Pal et al \cite{PAL19931277}. An even simpler method would be to use grey level threshold, where we segment 
        images depending on if they fall above or below some threshold, also discussed by Pal et al \cite{PAL19931277}.

        Finally, the method that we will use is to generate some pixel labelling $\cl{P}:\Omega \rightarrow \cl{C}$ as 
        described in section \ref{sec:what_are_we_trying_to_do}. This implicitly defines a segmentation of an image 
        via regions of similarly classed pixels.




    \section{Supervised learning for classification} \label{sec:supervised_learning}
        Before we discuss any machine learning methods we first need to define some terminology that we will be using. 
        In classification we are given a \textit{feature vector} (or \textit{instance}) $\vc{x} = (x_1, x_2, ..., x_d) 
        \in \bb{R}^d$ and a set of classes $\cl{C} = \{C_1, C_2, ..., C_n\}$. We want to \textit{learn} some 
        hypothesis $h:\bb{R}^d \rightarrow \cl{C}$ which maps instances to their classification. To learn 
        what hypothesis/hypotheses are appropriate we rely on a \textit{training sequence} $\vc{s} = ((\vc{x}_1, c_1), 
        (\vc{x}_2, c_2), ..., (\vc{x}_m, c_m))$, hence why the method is \textit{supervised}. We usually assume that 
        the training sequence is noisy, which can be sufficiently represented by noise just in the classes of $\vc{s}$, 
        that is $c_1, ..., c_m$ and so we can consider each of the $c_i$ to be a random variable, and hence consider 
        $\vc{s}$ to be a random variable. Two sensible approaches that we might take to picking some hypothesis $h$ 
        are picking

        \begin{align}
            h_{\ML} = \argmax_{h\in\cl{H}} \Pr(h | \vc{s})
        \end{align}

        or picking

        \begin{align}
            h_{\MAP} = \argmax_{h\in\cl{H}} \Pr(\vc{s} | h) \Pr(h).
        \end{align} 

        Where $\ML$ stands for \textit{maximum likelihood} and $\MAP$ stands for \textit{maximum a-posteriori}. The $\MAP$ 
        hypothesis allows the prior $\Pr(h)$ to be used (indicating some prior knowledge about the hypothesis), however 
        if $\Pr(h)$ is uniform, then the $\ML$ and $\MAP$ hypothesis are easily shown to be equivilent using Bayes' theorem. 
        These two methods of picking a hypothesis are referred to as Bayesian learning methods, as we look to maximise 
        likelihoods and we will see that this is not the only way to pick a suitable hypothesis. \cite{russell1995modern}

        We often also see that some hypothesis will take the form of $h':\bb{R}^d \rightarrow (\cl{C} \rightarrow [0,1])$, 
        that is it gives a probability distribution over classes. We can then simply set 
        
        \begin{align}
            h(\vc{x}) = \argmax_{c\in\cl{C}} h'(\vc{x})(c)
        \end{align}

        which is `the most likely class'.

        



    \section{Decision trees and random forests} \label{sec:rand_forest}
        We will now define what a decision tree is, how we can use it in supervised learning and then extend it onto 
        the concept of random forests. As mentioned in section \ref{sec:supervised_learning} we opt for a hypothesis 
        which outputs a probability distribution over classes as opposed to a single class, which can be used to give 
        a single class as mentioned previously.

        \subsection{Introduction to decision trees} \label{sec:intro_decision_trees}
            Decision trees consist of a simple binary tree structure. Intuitively we think about asking a question at each 
            node, makeing a decision on which child node to traverse to and eventually reach some conclusion at a leaf node. 
            We can use this model in the situation described in section \ref{sec:supervised_learning}, where we are allowed 
            to `ask a question' about the feature vector at each decision node and leaf nodes consider

            \begin{figure}[H]
                \centering

                \tikzset{QuestionNode/.style = { rectangle, rounded corners, shade, 
                              top color = white, bottom color = blue!50!black!20, 
                              draw = blue!40!black!60, very thick, text ragged },
                          EdgeNodeStyle/.style = {draw = none}, above,
                          EmptyNode/.style = {draw = none}}
                \begin{tikzpicture}
                    [
                        sibling distance = 3cm,
                        level distance = 4cm,
                        edge from parent/.style = {draw, arrows = ->},
                        level 1/.style = {sibling distance = 3cm, level distance = 3cm},
                        level 2/.style = {sibling distance = 1.5cm, level distance = 3cm},
                    ]
                    \node [QuestionNode] {Blond hair?}
                        child {
                            node [QuestionNode] {Brown hair?}
                            child {
                                node [EmptyNode] {$\vdots$}
                                edge from parent
                                  node [EdgeNodeStyle, left] {no}
                            }
                            child {
                                node [EmptyNode] {$\vdots$}
                                edge from parent
                                  node [EdgeNodeStyle, right] {yes}
                            }
                            edge from parent node [EdgeNodeStyle, left] {no}
                        }   
                        child {
                            node [QuestionNode] {Blue eyes?}
                            child {
                                node [EmptyNode] {$\vdots$}
                                edge from parent
                                  node [EdgeNodeStyle, left] {no}
                            }
                            child {
                                node [EmptyNode] {$\vdots$}
                                edge from parent
                                  node [EdgeNodeStyle, right] {yes}
                            }
                            edge from parent node [EdgeNodeStyle, right] {yes}
                        };
                    \end{tikzpicture}

                \caption{Example of (part of) a tree used to classify humans.}
            \end{figure}

            More formally we can define a decision tree $T$ for classification, which has a set of states $Q \subseteq 
            \bb{N}$, where if $i \in Q$ is a decision node, then it has children $2i, 2i+1 \in Q$. 

            \begin{figure}[H]
                \centering
                \includegraphics[scale=0.5]{tree_node_labels.jpeg}
                \caption{Example of node numberings in a decision tree.}
            \end{figure}

            For our model we specify some function $f:\bb{R}^d \times \cl{T} \rightarrow \bb{B}$, where $\cl{T}$ is some 
            parameter space and is used to specify a function from $\bb{R}^d$ to $\bb{B}$ at each decision node. We call 
            $f$ a \textit{weak learner} or a \textit{split function}. In a decision tree for each state $i \in Q$ we have 
            some associated $\vc{\theta}_i \in \cl{T}$ called a \textit{split parameter}, used to specify 
            $f_i(\vc{x}) = f(\vc{x}; \vc{\theta}_i)$. The function\footnote{$\bb{B}$ is defined in section 
            \ref{sec:term_and_def}} $f_i : \bb{R}^d \rightarrow \bb{B}$ is then used to make a decision to traverse to 
            state $2i$ or $2i+1$ next. Finally when we hit a leaf node $j \in Q$ we need to specify some probability 
            distribution $p_j:\cl{C} \rightarrow [0,1]$ where we say 

            \begin{align}
                p_j(c) = \Pr(\vc{x} \in c | \vc{x} \text{ traversed to } j \text{ in } T).
            \end{align}

            As can be seen in figure \ref{fig:decision_tree_classify} to classify some instance $\vc{x}$ using a 
            decision tree $T$ we simply traverse $T$ using $f_i$ at each node $i \in Q$ to make a decision whether 
            to traverse to $2i$ or $2i+1$ next. We usually use the rule if $f_i(\vc{x}) = 0$ then traverse to 
            $2i$, otherwise if $f_i(\vc{x}) = 1$ then traverse to $2i+1$. \cite{criminisi2013decision}

            \begin{figure}[H]
                \centering
                \includegraphics[scale=0.5]{tree_example_classification.jpeg}
                \caption{Example of how some instance $\vc{x}$ would be classified using a decision tree.}
                \label{fig:decision_tree_classify}
            \end{figure}

        \subsection{Training a decision tree}
            When we train a decision tree we use a greedy approach, we train at each node optimally. We must use a 
            greedy approach because optimising over all possible trees is a computationally infeasible task (it is an 
            NP-complete problem to decide if a tree is optimal). So consider having a set of states $Q$ and for each 
            $i \in Q$ wanting to learn the split parameter $\vc{\theta}_i \in \cl{T}$. Let $\vc{s}_i$ be the training 
            sequence that $i$ is trained on. It is sensible to let $\vc{s_0}=\vc{s}$. 

            We first define the \textit{entropy} of a training sequence

            \begin{align}
                H(s) = - \sum_{c\in\cl{C}} p(c) \log_2 (p(c)).
                \label{eq:training_seq_entropy}
            \end{align}

            We assume feature vectors in $s$ to be unique for ease of notation and let

            \begin{align}
                p(c) = \frac{\left| \{\vc{v}\ |\ c' = c \land (\vc{v},c') \in S \} \right|}{\left| S \right|},
                \label{eq:empirical_distribution}
            \end{align}
            
            which is the empirical probability of a training sample vector in $s$ being classified as $c$. Now we define 
            a left and right split of $\vc{s}$, using some split parameter $\vc{\theta} \in \cl{T}$ according to our weak 
            learner $f$ as follows

            \begin{align}
                L(\vc{s},\vc{\theta}) & = \{ (\vc{x}, c) \in s | f(\vc{x}, \vc{\theta}) = 0 \}  \label{eq:left_split}\\
                R(\vc{s},\vc{\theta}) & = \{ (\vc{x}, c) \in s | f(\vc{x}, \vc{\theta}) = 1 \}. \label{eq:right_split}
            \end{align}

            We then consider the \textit{information gain} $I$ for performing a split according to $\vc{\theta}$ of 

            \begin{align}
                I(\vc{s}, \vc{\theta}) = H(\vc{s}) - \frac{1}{|\vc{s}|} \left( |L(\vc{s}, \vc{\theta})| H(L(\vc{s}, \vc{\theta})) 
                                                                          + |R(\vc{s}, \vc{\theta})| H(R(\vc{s}, \vc{\theta})) \right).
                \label{eq:information_gain}
            \end{align} 

            We now have a way that we might want to pick $\vc{\theta}_i$ at each node

            \begin{align} 
                \vc{\theta}_i = \argmax_{\theta\in\cl{T}} I(\vc{s}_i, \vc{\theta}).
            \end{align} 

            Once we have found a $\vc{\theta}_i$ we then set $s_{2i} = L(\vc{s}_i, \vc{\theta}_i)$ and 
            $s_{2i+1} = R(\vc{s}_i, \vc{\theta}_i)$, which allows us to begin training at node $0$ with $\vc{s}_0 = \vc{s}$ 
            and then recursively train down the tree. \cite{criminisi2013decision}


        \subsection{Moving swiftly on to random forests}
            Decision trees have a couple major disadvantages, which are solved by extending the concept to random 
            forests. The first problem is that we learn $\vc{\theta}_i$ using

            \begin{align} 
                \vc{\theta}_i = \argmax_{\theta\in\cl{T}} I(\vc{s}_i, \vc{\theta}),
            \end{align}

            which is a problem because $|\cl{T}|$ may be infinite. We solve this problem by randomly sampling $\cl{T}$ with 
            $\rho$ values, so let $\cl{T}_\rho = \{ \vc{\theta}_i^{(1)}, ..., \vc{\theta}_i^{(\rho)} \} \subseteq \cl{T}$.
            So we instead set 

            \begin{align} 
                \vc{\theta}_i = \argmax_{\theta\in\cl{T}_\rho} I(\vc{s}_i, \vc{\theta}),
            \end{align}            

            when training at node $i$, and a \textit{fresh} $\rho$ samples are taken from $\cl{T}$ per node. This 
            optimisation is called the \textit{random node optimisation}. The second problem with decision trees is that 
            we can easily over fit and also we cannot solve some XOR/parity problems \cite{criminisi2013decision}. We 
            can solve this problem by using a \textit{forest} of decision trees. A forest $F$ is simply a set of trees, 
            that is $F = \{ T_1, T_2, ..., T_k \}$, where each $T_\ell$ is trained separately with training sequence 
            $\vc{s}$. As we randomly trained each tree on the same data, it is reasonable to say that the probability of 
            any given tree in $F$ being the `correct' tree is equally likely, hence we set $\Pr(T_\ell) = 1/k$ for each 
            $\ell$. Finally, when we wish to classify some instance $\vc{x}$ using a random forest we output 

            \begin{align}
                p(c) & = \Pr(\vc{x} \in c) \\
                    & = \sum_{\ell=1}^k \Pr(\vc{x} \in c, T_\ell) \\
                    & = \sum_{\ell=1}^k \Pr(\vc{x} \in c | T_\ell) \Pr(T_\ell) \\
                    & = \frac{1}{k} \sum_{\ell=1}^k \Pr(\vc{x} \in c | T_\ell)
            \end{align}

            and each $\Pr(\vc{x} \in c | T_\ell)$ is simply the probability given by traversing each tree. \cite{criminisi2013decision}

            Simply put, to classify using a forest, we just take an average of the outputs from the trees. 





    \section{Neural Networks}
        We now move on to a describe a second method of supervised learning, artificial neural networks. We will begin 
        by defining a single artificial neuron (referred to as just neurons and neural networks henceforth) and we look 
        at how it operates. We then move onto looking at a network of neurons and how we can vary the weights in the 
        network to train it. 

        \begin{framed}
            TODO
        \end{framed}





    \section{Imaging and image noise} 
        We now consider image sensor arrays, we want to know at a high level how images are captured and what might 
        cause image noise. We will the define what the quantum noise model is and why it is appropriate in many 
        cases of medical imaging. 

        \subsection{Image sensor arrays}
            All forms of digital imaging involve some form of sensor array. Two common technologies are used in image 
            sensors, Charge-Coupled Devices (CCD) and Complementary Metal-Oxide Semiconductor (CMOS). These devices are 
            laid out in a grid, which are typically combined with (colour) wavelength filters. 

            \begin{figure}[H]
                \centering
                \includegraphics[scale=0.30]{sensor_arrays}
                \caption{CCD and CMOS sensor arrays. Reproduced from Gamel et al \cite{gamal2005cmos}.}
                \label{fig:image_pipeline}
            \end{figure}

            \begin{figure}[H]
                \centering
                \includegraphics[scale=0.30]{imaging_pipeline}
                \caption{Overview of an imaging `pipeline'. Reproduced from Gamel et al \cite{gamal2005cmos}.}
                \label{fig:image_pipeline}
            \end{figure}

            In a camera we will have a focussing lens, followed by wavelength filters and then our sensors. So it 
            suffices to consider a model where we have parallel streams of photons hitting each sensor.

            \begin{figure}[H]
                \centering
                \includegraphics[scale=0.5]{photon_stream}
                \caption{Parallel stream of photons incident on one sensor in a sensor array.}
                \label{fig:photon_stream}
            \end{figure}

            For our purpose it is enough to consider that a stream of photons, will be hitting some photodiode (the 
            part of a sensor that induces a current when absorbing a photon), causing charge to collect on some 
            capacitor, which will increase linearly with respect to time at a rate proportional to the rate of photons 
            incident on the photoreceptor (the photocurrent), until it hits some maximum value. \cite{gamal2005cmos}

            \begin{figure}[H]
                \centering

                    \begin{tikzpicture}
                        \begin{axis}[
                            name = plot2,
                            axis lines = left,
                            xtick={0,1},
                            xticklabels={$\ $,$t_{\text{exposure}}$},
                            ytick={0,1},
                            yticklabels={$\ $,$Q_{\text{max}}$},
                            xlabel = $t$ (time),
                            ylabel = $Q$ (charge),
                            legend pos=outer north east,
                            xmin=0,
                            ymin=0,
                            xmax=1.1,
                            ymax=1.1
                        ]
                            \draw[dotted] (axis cs:0,1) -- (axis cs:1.2,1);
                            \draw[dotted] (axis cs:1,0) -- (axis cs:1,1.2);
                            \addplot[domain=0:0.75, samples = 5, red] {4 * x/3};
                            \addlegendentry{`High light'};
                            \addplot[domain=0:1, samples = 5, blue] {x/2};     %N.b. filthy haxx to get legend to show 1 red, 1 blue instead of 2 red
                            \addlegendentry{`Low light'};
                            \addplot[domain=0.75:1, samples = 5, red] {1};
                        \end{axis}

                    \end{tikzpicture}

                \caption{The charge held on a capacitor is linear with respect time, until it hits some maximum \cite{gamal2005cmos}.}
                \label{fig:linear_charge_wrt_photon_rate}
            \end{figure}


        \subsection{Quantum noise}
            \textit{Quantum noise}, \textit{Photon noise} or \textit{Poisson noise} can be thought of as a uncertainty 
            associated with the measurement of light, due to the quantised nature of electromagnetic waves and the 
            independence of photon detections \cite{hasinoff2014photon}. More mathematically suppose we have some 
            noiseless image $I(x,y)$, then we define an image corrupted by photon noise $\tilde{I}$ by

            \begin{align}
                \tilde{I}(x,y) \sim \text{Poisson}(I(x,y)).
            \end{align}

            So we have

            \begin{align}
                \Pr(\tilde{I}(x,y) = k) & = 
                    \begin{cases}
                        e^{-I(x,y)} \frac{I(x,y)^k}{k!} & 0 \leq k              \label{eq:image_poisson} \\
                        0 & \text{otherwise}
                    \end{cases} \\
                \bb{E}\left[ \tilde{I}(x,y) \right] & = I(x,y) \\
                \var\left[ \tilde{I}(x,y) \right] & = I(x,y).                 
            \end{align}

            We can see that this is appropriate considering figure \ref{fig:photon_stream} as a Poisson process, where 
            each event is a photon hitting the photoreceptor. Suppose the exposure time to the sensor is 
            $t_{\text{exposure}}$, the average rate of events is $\phi$ and that $N(t)$ is the number of events that 
            occur in the interval $[0,t]$, then we have $N(t) \sim \text{Poisson}(\phi t)$ \cite{ross2002probability}. 
            And so we have $N(t_{\text{exposure}}) \sim \text{Poisson}(\phi t_{\text{exposure}})$. From figure 
            \ref{fig:linear_charge_wrt_photon_rate} we can justifiably say that if $\phi$ is the rate/intensity of 
            photons hitting sensor at index $(x,y)$ in the array, then $I(x,y) \propto N(t_{\text{exposure}})$.
            
            The model is easily and obviously extended to spatial images by including spectral bin indices 

            \begin{align}
                \tilde{I}(x,y,\lambda) \sim \text{Poisson}(I(x,y,\lambda)).
            \end{align}

            Because of the filters (shown in figure \ref{fig:image_pipeline}) being spatially separated we can consider 
            each value in the datacube to be an independent random variable. This means that 

            \begin{align}
                (x,y,\lambda) \neq (x',y',\lambda') \Rightarrow 
                      \Pr(\tilde{I}(x,y,\lambda), \tilde{I}(x',y',\lambda')) = \Pr(\tilde{I}(x,y,\lambda)) \Pr(\tilde{I}(x',y',\lambda')).
            \end{align}

            However, in images the values are constrained to a finite range, typically we have that for each $x, y, 
            \lambda$ that $I(x,y,\lambda) \in \bb{N}_{256}$, but this however doesn't restrict the value of
            $\tilde{I}(x,y,\lambda)$ to $\bb{N}_{256}$. By considering figure \ref{fig:linear_charge_wrt_photon_rate} 
            we see that the charge becomes \textit{saturated} at the upper limit, that is if we increase the photon 
            intensity above the rate which achieves the maximum value, we still get the maximum value. To deal with this 
            it makes sense to define a \textit{Saturated Poisson} distribution. So we say $X \sim \text{SPoisson}(\mu, n)$ 
            if 

            \begin{align}
                \Pr(X = k) & = 
                    \begin{cases}
                        e^{\mu} \frac{\mu^k}{k!} & 0 \leq k < n-1 \\
                        \sum\limits_{i=n-1}^\infty e^{\mu} \frac{\mu^i}{i!} & k = n-1 \\
                        0 & \text{otherwise}.
                    \end{cases} 
            \end{align}

            \begin{figure}[H]
                \centering 
                \includegraphics[scale=0.5]{poisson_distributions}
                \caption{Example of Poisson($\mu$) and SPoisson($\mu$, 30) distributions, for $\mu = 5,15,25$.}
            \end{figure}

        \subsection{Additive White Gaussian Noise}
            TODO??

        \subsection{Medical imaging noise}
            We end this section by considering how we should use our noise models ing medical imaging. For many methods 
            of medical imaging we use ionising radiation that would be considered harmful given extended exposure times 
            \cite{picano2004sustainability}. Because of this there is an inherent trade off between exposure time (for 
            greater image clarity) and risk of causing damage \cite{sprawls1987physical}. Because we may potentially 
            use shorter exposure times, or we may be imaging something in low light we find that often the quantum noise 
            model is more appropriate and also cannot be approximated by Gaussian noise. However, there are many cases 
            such as ultrasound where we may want to consider more sophisticated noise models \cite{coupe2009nonlocal}, 
            but for the purpose of this dissertation we will restrict ourselves to the above.








    \section{TVMM Image De-Noising}
        \begin{framed}
        \begin{itemize}
            \item A method developed for additive white Gaussian noise \cite{figueiredo2006total}, which even when produced 
                is outperformed by wavelet methods, outperforms algorithms developed for Poisson noise \cite{rodrigues2008denoising}.
            \item Definition of total variation
            \item Definition of the optimisation problem and the $Q$ function used
            \item Some properties of the Q function, that allow us to find the optimal solution to the optimisation problem
            \item Outline of the procedure
            \item Requires solution of linear equations. Can be done using conjugate gradients (appendix B).
            \item Trying to solve an inherently ill-posed problem
        \end{itemize}
        \end{framed}



    \section{Requirements analysis}
        With the background reading out of the way, we move onto the design of the system. Firstly we carefully consider 
        the requirements of the system. This is important to begin identifying the work that needs to be undertaken 
        and where the potential risk lies. 

        \begin{table}[H]
            \begin{minipage}{\textwidth}
                \begin{tabular}{lccc}
                    \hline 
                        \textbf{Goal Requirement} & \textbf{Priority} & \textbf{Risk} & \textbf{Difficulty} \\
                    \hline 
                        Build a random forests machine learning library       & High      & Low         & Medium    \\
                        Incorporate a neural network library (Encog)          & High      & Medium      & Medium    \\
                        Image segmentation via pixel labelling                & High      & Low         & Low       \\
                        Suitably model and account for image noise            & Medium    & High        & High      \\ 
                        Train the pixel labelling system on real data         & High      & High        & Low       \\
                        Build an aid to help create training sets             & Medium    & Low         & Low       \\
                        Model uncertainty and output in a certainty map       & Medium    & Medium      & Medium    \\
%                        Optimise algorithms and implementations for GPU       & Low       & High        & High      \\
                    \hline  
                \end{tabular}
            \end{minipage}
            \caption{High level goals and desired outcomes for the project.}
        \end{table}


    \section{System Design}
        Here we outline the overview of the the whole system, where we begin with a work flow diagram comprising of 
        files and system components. We then divide the system into it's separate components and use it to define an 
        interface, along with a brief description of each module, defining inputs and outputs. The formats of the files 
        that the user should input can be found in appendix \ref{app:file_formats}.


        \subsection{Pipeline/Overview}
            We now define here an overview of what we want the system to do (the pipeline of the system) in terms of the 
            files input and output by different modules of the system. In the system overview below, blue boxes are code 
            modules and red boxes represent files.

            \begin{figure}[H]
                \centering
                    \scalebox{0.55}{ 
                        \begin{tikzpicture}[node distance=2cm,>=stealth',bend angle=45,auto]

                            \tikzstyle{commonNode}=[thick, font=\ttfamily, align=center, line width=2pt, draw=black]
                            \tikzstyle{module}=[commonNode, rectangle, rounded corners, fill=blue!30, inner sep=5mm]
                            \tikzstyle{file}=[commonNode, fill=red!30]
                            \tikzstyle{unusedModule}=[module, fill=blue!10, line width=0pt]
                            \tikzstyle{unusedFile}=[file, fill=red!10, line width=0pt]
                            \tikzstyle{intermediateFile}=[file, dash pattern=on 3pt off 3pt, line width=1pt]

                            \begin{scope}
                                % Spacing the nodes out
                                \def \hspacing {5}
                                \def \vspacing {3}
                                \def \indent {0.25}

                                % Background
                                %\fill[black!20, rounded corners=10] (-0.75*\hspacing, 0.5*\vspacing) rectangle (4.5*\hspacing, 5.5*\vspacing);

                                % Title
                                %\node at (-0.75*\hspacing+\indent, 5.5*\vspacing-\indent) [anchor=north west] {\textbf{System Overview}};

                                % Inputs
                                \node at (-1*\hspacing, 4.5*\vspacing) (EHIin) {};
                                \node at (-1*\hspacing, 4*\vspacing) (EILin) {};
                                \node at (-1*\hspacing, 3.5*\vspacing) (LMin) {};
                                \node at (-1*\hspacing, 2.75*\vspacing) (TSin) {};
                                \node at (-1*\hspacing, 2*\vspacing) (NHIin) {};
                                \node at (-1*\hspacing, 1*\vspacing) (HIin) {};

                                % Files and modules
                                \node at (0*\hspacing, 4.5*\vspacing) [file]    (EHI)     {Example \\ Spectral Image}
                                    edge  [pre]   (EHIin);
                                \node at (0*\hspacing, 4*\vspacing)   [file]    (EIL)     {Example Image \\ Labellings}
                                    edge  [pre]   (EILin);
                                \node at (0*\hspacing, 3.5*\vspacing) [file]    (LM)      {Label Map}
                                    edge  [pre]   (LMin);
                                \node at (1*\hspacing, 4*\vspacing)   [module]  (TT)      {Training \\ Tools}
                                    edge  [pre]   (EHI)
                                    edge  [pre]   (EIL)
                                    edge  [pre]   (LM);
                                \node at (1*\hspacing, 2.75*\vspacing)   [file]    (TS)      {Training \\ Sequence}
                                    edge  [pre]   (TSin)
                                    edge  [pre]   (TT);
                                \node at (3*\hspacing, 3*\vspacing)   [module]  (L)       {Learner}
                                    edge  [pre]   (LM)
                                    edge  [pre]   (TS);
                                \node at (2.5*\hspacing, 2*\vspacing) [file]    (F)       {Decision \\ Forest}
                                    edge  [pre]   (L);
                                \node at (3.5*\hspacing, 2*\vspacing) [file]    (NN)      {Neural \\ Network}
                                    edge  [pre]   (L);
                                \node at (0*\hspacing, 2*\vspacing)   [file]    (NHI)     {Noisy \\ Spectral Image}
                                    edge  [pre]   (NHIin);
                                \node at (1*\hspacing, 2*\vspacing)   [module]  (DB)      {DeNoising}
                                    edge  [pre]   (NHI);
                                \node at (1*\hspacing, 1*\vspacing)   [file]    (HI)      {Spectral Image}
                                    edge  [pre]   (HIin)
                                    edge  [pre]   (DB);
                                \node at (3*\hspacing, 1*\vspacing)   [module]  (C)       {Pixel Labeller}
                                    edge  [pre]   (F)
                                    edge  [pre]   (NN)
                                    edge  [pre]   (HI);
                                \node at (4*\hspacing, 1*\vspacing)   [file]    (PL)      {Pixel \\ Labelling}
                                    edge  [pre]   (C);
                            \end{scope}

                            % \begin{pgfonlayer}{background}
                                
                            % \end{pgfonlayer}
                        \end{tikzpicture}
                    }

                \caption{Overview of the system and its intended use.}
                \label{fig:system_overview}
            \end{figure}   



        \subsection{Interface/Usage}
            We will allow usage for each module separately, however we will also allow for combined steps, that will 
            avoid unnecessary intermediate files to be produced. For our interface we extend our diagrams that consisted 
            of red boxes for files and blue boxes for code modules, to denote files/modules unused in a given function 
            by a faded/duller colour, and we denote an intermediate file that isn't saved during some function by a 
            regularly bright, but dashed boarder. It should be assumed that an intermediate file need not be provided 
            by the user.




            \subsubsection{Training Tools}
                The training tools function is to take an example class map, pixel labelling and spectral image (see 
                appendix \ref{app:file_formats} for how their formats). From this `ground truth' we will output a 
                training sequence as specified in appendix \ref{app:file_formats}. The use of this function is that we 
                can generate vast quantities of training data with very little effort.

                \begin{figure}[H]
                    \centering
                        \scalebox{0.55}{ 
                            \begin{tikzpicture}[node distance=2cm,>=stealth',bend angle=45,auto]

                                \tikzstyle{commonNode}=[thick, font=\ttfamily, align=center, line width=2pt, draw=black]
                                \tikzstyle{module}=[commonNode, rectangle, rounded corners, fill=blue!30, inner sep=5mm]
                                \tikzstyle{file}=[commonNode, fill=red!30]
                                \tikzstyle{unusedModule}=[module, fill=blue!10, line width=1pt]
                                \tikzstyle{unusedFile}=[file, fill=red!10, line width=1pt]
                                \tikzstyle{intermediateFile}=[file, dash pattern=on 3pt off 3pt, line width=1pt]

                                \begin{scope}
                                    % Spacing the nodes out
                                    \def \hspacing {5}
                                    \def \vspacing {3}
                                    \def \indent {0.25}

                                    % Background
                                    %\fill[black!20, rounded corners=10] (-0.75*\hspacing, 0.5*\vspacing) rectangle (4.5*\hspacing, 5.5*\vspacing);

                                    % Title
                                    %\node at (-0.75*\hspacing+\indent, 5.5*\vspacing-\indent) [anchor=north west] {\textbf{System Overview}};

                                    % Inputs
                                    \node at (-1*\hspacing, 4.5*\vspacing) (EHIin) {};
                                    \node at (-1*\hspacing, 4*\vspacing) (EILin) {};
                                    \node at (-1*\hspacing, 3.5*\vspacing) (LMin) {};
                                    \node at (-1*\hspacing, 2.75*\vspacing) (TSin) {};
                                    \node at (-1*\hspacing, 2*\vspacing) (NHIin) {};
                                    \node at (-1*\hspacing, 1*\vspacing) (HIin) {};

                                    % Files and modules
                                    \node at (0*\hspacing, 4.5*\vspacing) [file]    (EHI)     {Example \\ Spectral Image}
                                        edge  [pre]   (EHIin);
                                    \node at (0*\hspacing, 4*\vspacing)   [file]    (EIL)     {Example Image \\ Labellings}
                                        edge  [pre]   (EILin);
                                    \node at (0*\hspacing, 3.5*\vspacing) [file]    (LM)      {Label Map}
                                        edge  [pre]   (LMin);
                                    \node at (1*\hspacing, 4*\vspacing)   [module]  (TT)      {Training \\ Tools}
                                        edge  [pre]   (EHI)
                                        edge  [pre]   (EIL)
                                        edge  [pre]   (LM);
                                    \node at (1*\hspacing, 2.75*\vspacing)   [file]    (TS)      {Training \\ Sequence}
                                        edge  [pre]   (TSin)
                                        edge  [pre]   (TT);
                                    \node at (3*\hspacing, 3*\vspacing)   [unusedModule]  (L)       {Learner}
                                        edge  [pre]   (LM)
                                        edge  [pre]   (TS);
                                    \node at (2.5*\hspacing, 2*\vspacing) [unusedFile]    (F)       {Decision \\ Forest}
                                        edge  [pre]   (L);
                                    \node at (3.5*\hspacing, 2*\vspacing) [unusedFile]    (NN)      {Neural \\ Network}
                                        edge  [pre]   (L);
                                    \node at (0*\hspacing, 2*\vspacing)   [unusedFile]    (NHI)     {Noisy \\ Spectral Image}
                                        edge  [pre]   (NHIin);
                                    \node at (1*\hspacing, 2*\vspacing)   [unusedModule]  (DB)      {DeNoising}
                                        edge  [pre]   (NHI);
                                    \node at (1*\hspacing, 1*\vspacing)   [unusedFile]    (HI)      {Spectral Image}
                                        edge  [pre]   (HIin)
                                        edge  [pre]   (DB);
                                    \node at (3*\hspacing, 1*\vspacing)   [unusedModule]  (C)       {Pixel Labeller}
                                        edge  [pre]   (F)
                                        edge  [pre]   (NN)
                                        edge  [pre]   (HI);
                                    \node at (4*\hspacing, 1*\vspacing)   [unusedFile]    (PL)      {Pixel \\ Labelling}
                                        edge  [pre]   (C);
                                \end{scope}

                                % \begin{pgfonlayer}{background}
                                    
                                % \end{pgfonlayer}
                            \end{tikzpicture}
                        }

                    \caption{Overview of the training tools function.}
                \end{figure} 



            \subsubsection{Train}
                The train function will take a training sequence and then produce either a forest file or a neural 
                network file. These files will be output to be used later by the pixel labeller.

                \begin{figure}[H]
                    \centering
                        \scalebox{0.55}{ 
                            \begin{tikzpicture}[node distance=2cm,>=stealth',bend angle=45,auto]

                                \tikzstyle{commonNode}=[thick, font=\ttfamily, align=center, line width=2pt, draw=black]
                                \tikzstyle{module}=[commonNode, rectangle, rounded corners, fill=blue!30, inner sep=5mm]
                                \tikzstyle{file}=[commonNode, fill=red!30]
                                \tikzstyle{unusedModule}=[module, fill=blue!10, line width=1pt]
                                \tikzstyle{unusedFile}=[file, fill=red!10, line width=1pt]
                                \tikzstyle{intermediateFile}=[file, dash pattern=on 3pt off 3pt, line width=1pt]

                                \begin{scope}
                                    % Spacing the nodes out
                                    \def \hspacing {5}
                                    \def \vspacing {3}
                                    \def \indent {0.25}

                                    % Background
                                    %\fill[black!20, rounded corners=10] (-0.75*\hspacing, 0.5*\vspacing) rectangle (4.5*\hspacing, 5.5*\vspacing);

                                    % Title
                                    %\node at (-0.75*\hspacing+\indent, 5.5*\vspacing-\indent) [anchor=north west] {\textbf{System Overview}};

                                    % Inputs
                                    \node at (-1*\hspacing, 4.5*\vspacing) (EHIin) {};
                                    \node at (-1*\hspacing, 4*\vspacing) (EILin) {};
                                    \node at (-1*\hspacing, 3.5*\vspacing) (LMin) {};
                                    \node at (-1*\hspacing, 2.75*\vspacing) (TSin) {};
                                    \node at (-1*\hspacing, 2*\vspacing) (NHIin) {};
                                    \node at (-1*\hspacing, 1*\vspacing) (HIin) {};

                                    % Files and modules
                                    \node at (0*\hspacing, 4.5*\vspacing) [unusedFile]    (EHI)     {Example \\ Spectral Image}
                                        edge  [pre]   (EHIin);
                                    \node at (0*\hspacing, 4*\vspacing)   [unusedFile]    (EIL)     {Example Image \\ Labellings}
                                        edge  [pre]   (EILin);
                                    \node at (0*\hspacing, 3.5*\vspacing) [unusedFile]    (LM)      {Label Map}
                                        edge  [pre]   (LMin);
                                    \node at (1*\hspacing, 4*\vspacing)   [unusedModule]  (TT)      {Training \\ Tools}
                                        edge  [pre]   (EHI)
                                        edge  [pre]   (EIL)
                                        edge  [pre]   (LM);
                                    \node at (1*\hspacing, 2.75*\vspacing)   [file]    (TS)      {Training \\ Sequence}
                                        edge  [pre]   (TSin)
                                        edge  [pre]   (TT);
                                    \node at (3*\hspacing, 3*\vspacing)   [module]  (L)       {Learner}
                                        edge  [pre]   (LM)
                                        edge  [pre]   (TS);
                                    \node at (2.5*\hspacing, 2*\vspacing) [file]    (F)       {Decision \\ Forest}
                                        edge  [pre]   (L);
                                    \node at (3.5*\hspacing, 2*\vspacing) [file]    (NN)      {Neural \\ Network}
                                        edge  [pre]   (L);
                                    \node at (0*\hspacing, 2*\vspacing)   [unusedFile]    (NHI)     {Noisy \\ Spectral Image}
                                        edge  [pre]   (NHIin);
                                    \node at (1*\hspacing, 2*\vspacing)   [unusedModule]  (DB)      {DeNoising}
                                        edge  [pre]   (NHI);
                                    \node at (1*\hspacing, 1*\vspacing)   [unusedFile]    (HI)      {Spectral Image}
                                        edge  [pre]   (HIin)
                                        edge  [pre]   (DB);
                                    \node at (3*\hspacing, 1*\vspacing)   [unusedModule]  (C)       {Pixel Labeller}
                                        edge  [pre]   (F)
                                        edge  [pre]   (NN)
                                        edge  [pre]   (HI);
                                    \node at (4*\hspacing, 1*\vspacing)   [unusedFile]    (PL)      {Pixel \\ Labelling}
                                        edge  [pre]   (C);
                                \end{scope}

                                % \begin{pgfonlayer}{background}
                                    
                                % \end{pgfonlayer}
                            \end{tikzpicture}
                        }

                    \caption{Overview of the train function.}
                \end{figure} 



            \subsubsection{Learn From Example}
                The learn from example function is a composite of the training tools and train functions. This will 
                take the same inputs as the training tools function, but instead will immediately use the training 
                sequence produced to train a random forest or neural network, \textit{not} saving the training 
                sequence in the process.

                \begin{figure}[H]
                    \centering
                        \scalebox{0.55}{ 
                            \begin{tikzpicture}[node distance=2cm,>=stealth',bend angle=45,auto]

                                \tikzstyle{commonNode}=[thick, font=\ttfamily, align=center, line width=2pt, draw=black]
                                \tikzstyle{module}=[commonNode, rectangle, rounded corners, fill=blue!30, inner sep=5mm]
                                \tikzstyle{file}=[commonNode, fill=red!30]
                                \tikzstyle{unusedModule}=[module, fill=blue!10, line width=1pt]
                                \tikzstyle{unusedFile}=[file, fill=red!10, line width=1pt]
                                \tikzstyle{intermediateFile}=[file, dash pattern=on 3pt off 3pt, line width=1pt]

                                \begin{scope}
                                    % Spacing the nodes out
                                    \def \hspacing {5}
                                    \def \vspacing {3}
                                    \def \indent {0.25}

                                    % Background
                                    %\fill[black!20, rounded corners=10] (-0.75*\hspacing, 0.5*\vspacing) rectangle (4.5*\hspacing, 5.5*\vspacing);

                                    % Title
                                    %\node at (-0.75*\hspacing+\indent, 5.5*\vspacing-\indent) [anchor=north west] {\textbf{System Overview}};

                                    % Inputs
                                    \node at (-1*\hspacing, 4.5*\vspacing) (EHIin) {};
                                    \node at (-1*\hspacing, 4*\vspacing) (EILin) {};
                                    \node at (-1*\hspacing, 3.5*\vspacing) (LMin) {};
                                    \node at (-1*\hspacing, 2.75*\vspacing) (TSin) {};
                                    \node at (-1*\hspacing, 2*\vspacing) (NHIin) {};
                                    \node at (-1*\hspacing, 1*\vspacing) (HIin) {};

                                    % Files and modules
                                    \node at (0*\hspacing, 4.5*\vspacing) [file]    (EHI)     {Example \\ Spectral Image}
                                        edge  [pre]   (EHIin);
                                    \node at (0*\hspacing, 4*\vspacing)   [file]    (EIL)     {Example Image \\ Labellings}
                                        edge  [pre]   (EILin);
                                    \node at (0*\hspacing, 3.5*\vspacing) [file]    (LM)      {Label Map}
                                        edge  [pre]   (LMin);
                                    \node at (1*\hspacing, 4*\vspacing)   [module]  (TT)      {Training \\ Tools}
                                        edge  [pre]   (EHI)
                                        edge  [pre]   (EIL)
                                        edge  [pre]   (LM);
                                    \node at (1*\hspacing, 2.75*\vspacing)   [intermediateFile]    (TS)      {Training \\ Sequence}
                                        edge  [pre]   (TSin)
                                        edge  [pre]   (TT);
                                    \node at (3*\hspacing, 3*\vspacing)   [module]  (L)       {Learner}
                                        edge  [pre]   (LM)
                                        edge  [pre]   (TS);
                                    \node at (2.5*\hspacing, 2*\vspacing) [file]    (F)       {Decision \\ Forest}
                                        edge  [pre]   (L);
                                    \node at (3.5*\hspacing, 2*\vspacing) [file]    (NN)      {Neural \\ Network}
                                        edge  [pre]   (L);
                                    \node at (0*\hspacing, 2*\vspacing)   [unusedFile]    (NHI)     {Noisy \\ Spectral Image}
                                        edge  [pre]   (NHIin);
                                    \node at (1*\hspacing, 2*\vspacing)   [unusedModule]  (DB)      {DeNoising}
                                        edge  [pre]   (NHI);
                                    \node at (1*\hspacing, 1*\vspacing)   [unusedFile]    (HI)      {Spectral Image}
                                        edge  [pre]   (HIin)
                                        edge  [pre]   (DB);
                                    \node at (3*\hspacing, 1*\vspacing)   [unusedModule]  (C)       {Pixel Labeller}
                                        edge  [pre]   (F)
                                        edge  [pre]   (NN)
                                        edge  [pre]   (HI);
                                    \node at (4*\hspacing, 1*\vspacing)   [unusedFile]    (PL)      {Pixel \\ Labelling}
                                        edge  [pre]   (C);
                                \end{scope}

                                % \begin{pgfonlayer}{background}
                                    
                                % \end{pgfonlayer}
                            \end{tikzpicture}
                        }

                    \caption{Overview of the learn from example function.}
                \end{figure} 




            \subsubsection{Pixel Labeller}
                The pixel labeller could be considered the `main' part of the system. It uses the pixel labeller 
                module to label a spectral image using a trained forest/neural network.

                \begin{figure}[H]
                    \centering
                        \scalebox{0.55}{ 
                            \begin{tikzpicture}[node distance=2cm,>=stealth',bend angle=45,auto]

                                \tikzstyle{commonNode}=[thick, font=\ttfamily, align=center, line width=2pt, draw=black]
                                \tikzstyle{module}=[commonNode, rectangle, rounded corners, fill=blue!30, inner sep=5mm]
                                \tikzstyle{file}=[commonNode, fill=red!30]
                                \tikzstyle{unusedModule}=[module, fill=blue!10, line width=1pt]
                                \tikzstyle{unusedFile}=[file, fill=red!10, line width=1pt]
                                \tikzstyle{intermediateFile}=[file, dash pattern=on 3pt off 3pt, line width=1pt]

                                \begin{scope}
                                    % Spacing the nodes out
                                    \def \hspacing {5}
                                    \def \vspacing {3}
                                    \def \indent {0.25}

                                    % Background
                                    %\fill[black!20, rounded corners=10] (-0.75*\hspacing, 0.5*\vspacing) rectangle (4.5*\hspacing, 5.5*\vspacing);

                                    % Title
                                    %\node at (-0.75*\hspacing+\indent, 5.5*\vspacing-\indent) [anchor=north west] {\textbf{System Overview}};

                                    % Inputs
                                    \node at (-1*\hspacing, 4.5*\vspacing) (EHIin) {};
                                    \node at (-1*\hspacing, 4*\vspacing) (EILin) {};
                                    \node at (-1*\hspacing, 3.5*\vspacing) (LMin) {};
                                    \node at (-1*\hspacing, 2.75*\vspacing) (TSin) {};
                                    \node at (-1*\hspacing, 2*\vspacing) (NHIin) {};
                                    \node at (-1*\hspacing, 1*\vspacing) (HIin) {};

                                    % Files and modules
                                    \node at (0*\hspacing, 4.5*\vspacing) [unusedFile]    (EHI)     {Example \\ Spectral Image}
                                        edge  [pre]   (EHIin);
                                    \node at (0*\hspacing, 4*\vspacing)   [unusedFile]    (EIL)     {Example Image \\ Labellings}
                                        edge  [pre]   (EILin);
                                    \node at (0*\hspacing, 3.5*\vspacing) [unusedFile]    (LM)      {Label Map}
                                        edge  [pre]   (LMin);
                                    \node at (1*\hspacing, 4*\vspacing)   [unusedModule]  (TT)      {Training \\ Tools}
                                        edge  [pre]   (EHI)
                                        edge  [pre]   (EIL)
                                        edge  [pre]   (LM);
                                    \node at (1*\hspacing, 2.75*\vspacing)   [unusedFile]    (TS)      {Training \\ Sequence}
                                        edge  [pre]   (TSin)
                                        edge  [pre]   (TT);
                                    \node at (3*\hspacing, 3*\vspacing)   [unusedModule]  (L)       {Learner}
                                        edge  [pre]   (LM)
                                        edge  [pre]   (TS);
                                    \node at (2.5*\hspacing, 2*\vspacing) [file]    (F)       {Decision \\ Forest}
                                        edge  [pre]   (L);
                                    \node at (3.5*\hspacing, 2*\vspacing) [file]    (NN)      {Neural \\ Network}
                                        edge  [pre]   (L);
                                    \node at (0*\hspacing, 2*\vspacing)   [unusedFile]    (NHI)     {Noisy \\ Spectral Image}
                                        edge  [pre]   (NHIin);
                                    \node at (1*\hspacing, 2*\vspacing)   [unusedModule]  (DB)      {DeNoising}
                                        edge  [pre]   (NHI);
                                    \node at (1*\hspacing, 1*\vspacing)   [file]    (HI)      {Spectral Image}
                                        edge  [pre]   (HIin)
                                        edge  [pre]   (DB);
                                    \node at (3*\hspacing, 1*\vspacing)   [module]  (C)       {Pixel Labeller}
                                        edge  [pre]   (F)
                                        edge  [pre]   (NN)
                                        edge  [pre]   (HI);
                                    \node at (4*\hspacing, 1*\vspacing)   [file]    (PL)      {Pixel \\ Labelling}
                                        edge  [pre]   (C);
                                \end{scope}

                                % \begin{pgfonlayer}{background}
                                    
                                % \end{pgfonlayer}
                            \end{tikzpicture}
                        }

                    \caption{Overview of the pixel labeller function.}
                \end{figure} 




            \subsubsection{De-Noiser}
                The de-noiser function simply attempts to reduce the effect of noise on the image. It takes a spectral 
                image as input and outputs the de-noised spectral image.

                \begin{figure}[H]
                    \centering
                        \scalebox{0.55}{ 
                            \begin{tikzpicture}[node distance=2cm,>=stealth',bend angle=45,auto]

                                \tikzstyle{commonNode}=[thick, font=\ttfamily, align=center, line width=2pt, draw=black]
                                \tikzstyle{module}=[commonNode, rectangle, rounded corners, fill=blue!30, inner sep=5mm]
                                \tikzstyle{file}=[commonNode, fill=red!30]
                                \tikzstyle{unusedModule}=[module, fill=blue!10, line width=1pt]
                                \tikzstyle{unusedFile}=[file, fill=red!10, line width=1pt]
                                \tikzstyle{intermediateFile}=[file, dash pattern=on 3pt off 3pt, line width=1pt]

                                \begin{scope}
                                    % Spacing the nodes out
                                    \def \hspacing {5}
                                    \def \vspacing {3}
                                    \def \indent {0.25}

                                    % Background
                                    %\fill[black!20, rounded corners=10] (-0.75*\hspacing, 0.5*\vspacing) rectangle (4.5*\hspacing, 5.5*\vspacing);

                                    % Title
                                    %\node at (-0.75*\hspacing+\indent, 5.5*\vspacing-\indent) [anchor=north west] {\textbf{System Overview}};

                                    % Inputs
                                    \node at (-1*\hspacing, 4.5*\vspacing) (EHIin) {};
                                    \node at (-1*\hspacing, 4*\vspacing) (EILin) {};
                                    \node at (-1*\hspacing, 3.5*\vspacing) (LMin) {};
                                    \node at (-1*\hspacing, 2.75*\vspacing) (TSin) {};
                                    \node at (-1*\hspacing, 2*\vspacing) (NHIin) {};
                                    \node at (-1*\hspacing, 1*\vspacing) (HIin) {};

                                    % Files and modules
                                    \node at (0*\hspacing, 4.5*\vspacing) [unusedFile]    (EHI)     {Example \\ Spectral Image}
                                        edge  [pre]   (EHIin);
                                    \node at (0*\hspacing, 4*\vspacing)   [unusedFile]    (EIL)     {Example Image \\ Labellings}
                                        edge  [pre]   (EILin);
                                    \node at (0*\hspacing, 3.5*\vspacing) [unusedFile]    (LM)      {Label Map}
                                        edge  [pre]   (LMin);
                                    \node at (1*\hspacing, 4*\vspacing)   [unusedModule]  (TT)      {Training \\ Tools}
                                        edge  [pre]   (EHI)
                                        edge  [pre]   (EIL)
                                        edge  [pre]   (LM);
                                    \node at (1*\hspacing, 2.75*\vspacing)   [unusedFile]    (TS)      {Training \\ Sequence}
                                        edge  [pre]   (TSin)
                                        edge  [pre]   (TT);
                                    \node at (3*\hspacing, 3*\vspacing)   [unusedModule]  (L)       {Learner}
                                        edge  [pre]   (LM)
                                        edge  [pre]   (TS);
                                    \node at (2.5*\hspacing, 2*\vspacing) [unusedFile]    (F)       {Decision \\ Forest}
                                        edge  [pre]   (L);
                                    \node at (3.5*\hspacing, 2*\vspacing) [unusedFile]    (NN)      {Neural \\ Network}
                                        edge  [pre]   (L);
                                    \node at (0*\hspacing, 2*\vspacing)   [file]    (NHI)     {Noisy \\ Spectral Image}
                                        edge  [pre]   (NHIin);
                                    \node at (1*\hspacing, 2*\vspacing)   [module]  (DB)      {DeNoising}
                                        edge  [pre]   (NHI);
                                    \node at (1*\hspacing, 1*\vspacing)   [file]    (HI)      {Spectral Image}
                                        edge  [pre]   (HIin)
                                        edge  [pre]   (DB);
                                    \node at (3*\hspacing, 1*\vspacing)   [unusedModule]  (C)       {Pixel Labeller}
                                        edge  [pre]   (F)
                                        edge  [pre]   (NN)
                                        edge  [pre]   (HI);
                                    \node at (4*\hspacing, 1*\vspacing)   [unusedFile]    (PL)      {Pixel \\ Labelling}
                                        edge  [pre]   (C);
                                \end{scope}

                                % \begin{pgfonlayer}{background}
                                    
                                % \end{pgfonlayer}
                            \end{tikzpicture}
                        }

                    \caption{Overview of the de-noiser function.}
                \end{figure} 




            \subsubsection{Noisy Pixel Labeller}
                The noisy pixel labeller is a composite of the de-noiser and pixel labeller functions. We take a noisy 
                spectral image as input, along with a trained forest or neural network. De-noising is run on the image 
                before plugging the spectral image into the pixel labeller.

                \begin{figure}[H]
                    \centering
                        \scalebox{0.55}{ 
                            \begin{tikzpicture}[node distance=2cm,>=stealth',bend angle=45,auto]

                                \tikzstyle{commonNode}=[thick, font=\ttfamily, align=center, line width=2pt, draw=black]
                                \tikzstyle{module}=[commonNode, rectangle, rounded corners, fill=blue!30, inner sep=5mm]
                                \tikzstyle{file}=[commonNode, fill=red!30]
                                \tikzstyle{unusedModule}=[module, fill=blue!10, line width=1pt]
                                \tikzstyle{unusedFile}=[file, fill=red!10, line width=1pt]
                                \tikzstyle{intermediateFile}=[file, dash pattern=on 3pt off 3pt, line width=1pt]

                                \begin{scope}
                                    % Spacing the nodes out
                                    \def \hspacing {5}
                                    \def \vspacing {3}
                                    \def \indent {0.25}

                                    % Background
                                    %\fill[black!20, rounded corners=10] (-0.75*\hspacing, 0.5*\vspacing) rectangle (4.5*\hspacing, 5.5*\vspacing);

                                    % Title
                                    %\node at (-0.75*\hspacing+\indent, 5.5*\vspacing-\indent) [anchor=north west] {\textbf{System Overview}};

                                    % Inputs
                                    \node at (-1*\hspacing, 4.5*\vspacing) (EHIin) {};
                                    \node at (-1*\hspacing, 4*\vspacing) (EILin) {};
                                    \node at (-1*\hspacing, 3.5*\vspacing) (LMin) {};
                                    \node at (-1*\hspacing, 2.75*\vspacing) (TSin) {};
                                    \node at (-1*\hspacing, 2*\vspacing) (NHIin) {};
                                    \node at (-1*\hspacing, 1*\vspacing) (HIin) {};

                                    % Files and modules
                                    \node at (0*\hspacing, 4.5*\vspacing) [unusedFile]    (EHI)     {Example \\ Spectral Image}
                                        edge  [pre]   (EHIin);
                                    \node at (0*\hspacing, 4*\vspacing)   [unusedFile]    (EIL)     {Example Image \\ Labellings}
                                        edge  [pre]   (EILin);
                                    \node at (0*\hspacing, 3.5*\vspacing) [unusedFile]    (LM)      {Label Map}
                                        edge  [pre]   (LMin);
                                    \node at (1*\hspacing, 4*\vspacing)   [unusedModule]  (TT)      {Training \\ Tools}
                                        edge  [pre]   (EHI)
                                        edge  [pre]   (EIL)
                                        edge  [pre]   (LM);
                                    \node at (1*\hspacing, 2.75*\vspacing)   [unusedFile]    (TS)      {Training \\ Sequence}
                                        edge  [pre]   (TSin)
                                        edge  [pre]   (TT);
                                    \node at (3*\hspacing, 3*\vspacing)   [unusedModule]  (L)       {Learner}
                                        edge  [pre]   (LM)
                                        edge  [pre]   (TS);
                                    \node at (2.5*\hspacing, 2*\vspacing) [file]    (F)       {Decision \\ Forest}
                                        edge  [pre]   (L);
                                    \node at (3.5*\hspacing, 2*\vspacing) [file]    (NN)      {Neural \\ Network}
                                        edge  [pre]   (L);
                                    \node at (0*\hspacing, 2*\vspacing)   [file]    (NHI)     {Noisy \\ Spectral Image}
                                        edge  [pre]   (NHIin);
                                    \node at (1*\hspacing, 2*\vspacing)   [module]  (DB)      {DeNoising}
                                        edge  [pre]   (NHI);
                                    \node at (1*\hspacing, 1*\vspacing)   [intermediateFile]    (HI)      {Spectral Image}
                                        edge  [pre]   (HIin)
                                        edge  [pre]   (DB);
                                    \node at (3*\hspacing, 1*\vspacing)   [module]  (C)       {Pixel Labeller}
                                        edge  [pre]   (F)
                                        edge  [pre]   (NN)
                                        edge  [pre]   (HI);
                                    \node at (4*\hspacing, 1*\vspacing)   [file]    (PL)      {Pixel \\ Labelling}
                                        edge  [pre]   (C);
                                \end{scope}

                                % \begin{pgfonlayer}{background}
                                    
                                % \end{pgfonlayer}
                            \end{tikzpicture}
                        }

                    \caption{Overview of the noisy pixel labeller function.}
                \end{figure} 







    \section{Languages and tools}
        In this section we briefly describe the languages, libraries and tools that we used for the project. \\

        \noindent\textbf{Programming Languages and Libraries}
            \begin{description}[font=\normalfont\itshape, labelindent=10pt]
                \item[Java:] provides an OS independent language and is object oriented to allow for modular design.
                \item[JUnit:] a Java library used for unit testing, used for white box and black box testing throughout 
                    the development of the system. Unit tests are essential to find bugs and prevent their re-introduction.
                \item[Encog:] an easy to use neural networks library in Java/C\# written by Heaton research \cite{JMLR:v16:heaton15a}. 
            \end{description}

        \noindent\textbf{Integrated development environment}
            \begin{description}[font=\normalfont\itshape, labelindent=10pt]
                \item[Eclipse:] allows for rapid development in Java and integrates easily with libraries and tools such 
                    as JUnit and Git.
                \item[EclEmma:] an Eclipse plug-in the works with JUnit which highlights lines to indicate code coverage 
                    of unit tests.
                \item[ObjectAid:] another Eclipse plug-in, used to create UML class diagrams.
            \end{description}

        \noindent\textbf{Statistical analysis and visualisation}
            \begin{description}[font=\normalfont\itshape, labelindent=10pt]
                \item[Matlab:] an easy to use, extensible statistical package used for producing graphs.
            \end{description}

        \noindent\textbf{Document preparation}
            \begin{description}[font=\normalfont\itshape, labelindent=10pt]
                \item[\LaTeX:] used for typesetting this dissertation in a precise manor allowing control over most 
                    aspects of document layout and style.
                \item[Tikz:] a \LaTeX\ library useful for producing diagrams, such as figure \ref{fig:system_overview}.
                \item[Listings:] a \LaTeX\ library allowing colour formatted code for a plethora of languages.
                \item[Adobe Photoshop:] image editing software used to produce some diagrams.
            \end{description}

        \noindent\textbf{Version Control}
            \begin{description}[font=\normalfont\itshape, labelindent=10pt]
                \item[Git:] allows code to be developed systematically and rollback if necessary, as well as providing  
                    the ability to fork my core repository to try different strategies.
            \end{description}

        \noindent\textbf{Backup}
            \begin{description}[font=\normalfont\itshape, labelindent=10pt]
                \item[DropBox and Google Drive:] repositories were kept in both DropBox and Google Drive folders, which 
                    synced every time a file was changed.
                \item[Github:] provides a remote Git repository to store code, so also helps in provision of version 
                    control. The repository was updated every time a stable commit was made.
                \item[Time machine:] Apple's automatic backup system, allowing for hourly backups of the whole hard drive 
                    to be taken.
            \end{description}



    \section{Software engineering techniques}
        \subsection{Development model}
            After the design of the system it was fairly obvious that a mixture of iterative and waterfall models. 
            The system would have to be implemented in a modular form (the modules can be seen in figure 
            \ref{fig:system_overview}). Each module then needed to be implemented and was tested rigorously before 
            moving onto the next, which is important because of dependencies between modules. For each module it was 
            possible to manually produce the input files necessary to perform testing.

            Once a working system produce was produced, tested and we have confidence that it is correct, additional 
            features and improvements to the system were added in an iterative manor, similar to a spiral model. Unit 
            tests were used to make sure that any new additions don't break the existing, working parts of the system.

        \subsection{Testing}
            The development was test driven, and used a mixture of testing techniques to write unit tests. All tests 
            were written as unit tests so that they could be reused later to make sure that the system still works.
            The following methodologies were employed:
            \begin{description}[font=\normalfont\itshape, labelindent=10pt]
                \item[Black box testing:] this is when then internal system design not taken into account, used to make sure 
                    that the system works as a `black box', in terms of the expected input/outputs we specified in the 
                    design. To assure that internal system design wasn't taken into account these were written 
                    \textit{before} any code was written.
                \item[White box testing:] this is when the internal system design is taken into account, tests are designed so that every 
                    line of code is checked using the unit tests. This is difficult to check manually, so Eclipse plug-in 
                    EclEmma was used, a code coverage tool that works with JUnit.
                \item[Input sanitisation testing:] additional unit tests were written to make sure that erroneous inputs 
                    were handled correctly.
                \item[Incremental integration testing:] this refers to using a bottom up approach, testing functionality 
                    as it is implemented.
                \item[Usability testing:] a few users were asked to try use the system given only instructions in a readme 
                    file, the feedback from this was vital and shaped the design of the training tools module.
            \end{description}

        \subsection{Backup Plan}
            It is important to make sure that we have an effective backup plan to avoid any software, hardware or 
            user error that may result in a loss of work, something which is important to make sure that we do not 
            loose a years worth of work. 

            To make sure that no work was lost a number of systems were set in place to not only regularly back up 
            any work but also provide rollback if necessary. Google Drive and DropBox were employed to keep shadow 
            copies of the project directory, GitHub was used as a remote repository which was pushed to regularly. 
            Finally Apple's time machine software was also used to automatically take backups of the whole local 
            file system every hour.

            \begin{figure}[H]
                \centering
                \includegraphics[scale=0.75]{backup_plan}
                \caption[Overview of the backup strategy employed.]{Overview of the backup strategy employed\footnotemark.}
            \end{figure}
            \footnotetext{Image modified from: 
                \href{http://www.apple.com/airport-time-capsule/}{apple.com}, 
                \href{https://www.google.com/drive/}{google.com}, 
                \href{https://www.dropbox.com/business/why-dropbox-for-business}{dropbox.com}, 
                \href{https://github.com/}{github.com}, 
                \href{http://www.clipartpanda.com/categories/mac-computer-clip-art}{clipartpanda.com}}.





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% the implementation

\cleardoublepage
\chapter{Implementation}
    We begin this chapter by elaborating on the implementation of the Random Forests algorithm and the use of the Encog, a 
    neural network library for Java. We then proceed to use the supervised learning methods to build a procedure for 
    producing a pixel labelling from a spectral image. We ignore until later the problems of how we will produce 
    training data for the supervised learning methods, as these likely need to be extracted from an example labelling, 
    a problem that will be addressed in section \ref{sec:training_tools}. Finally we also wait until nearer the end of 
    the chapter to deal with image noise in section \ref{sec:de-noising}.

    The project is modularised into a number of packages, each building on top of each other. Dependencies can be 
    visualised in figure \ref{fig:package_dependencies}. We will use UML class diagrams\footnote{http://www.uml-diagrams.org/class-reference.html is a good reference for UML class diagrams} to show the 
    overview of classes within each package.

    \begin{figure}[H]
        \centering
        \includegraphics[scale=0.5]{package_dependencies.jpeg}
        \caption{The dependencies between different packages/modules in the system. Although TrainingTools package is
        capable of producing training sequence files both the NeuralNetworks and RandomDecisionForests, it only 
        \textit{depends} on RandomDecisionForests as it uses the class \texttt{TrainingSequence}.}
        \label{fig:package_dependencies}
    \end{figure}




    \section{Random Forests Library}
        We begin by describing our implementation of random forests, looking specifically at some critical design 
        choices that were made. We implement a generic and easily extensible random forests library, which can 
        be specialised for many applications, then write specialising subclasses that utilise the library for our 
        purpose of providing a pixel labelling.

        \begin{figure}[H]
            \centering
            \includegraphics[scale=0.25, angle=90]{Forest_UML}
            \caption{An overview of classes in the RandomDecisionForest package.}
            \label{fig:forest_uml}
        \end{figure}

        \subsection{Data structures}
            A number of important data structures are used throughout the library, each requiring an efficient 
            implementation and providing necessary abstraction and before we move onto the more complicated issues 
            regarding supervised learning we need to describe the structures used. In this subsection 
            we will describe the main classes that are used in the RandomDecisionForest package, providing a table of 
            functions defined with corresponding English descriptions of what they do and some small code listings 
            where necessary.



            \subsubsection{ClassLabels}
                Our first class, \texttt{ClassLabels}, abstracts classification labels into its own class. As can be 
                seen in figure \ref{fig:class_label_uml} \texttt{ClassLabels} groups together a class name and its 
                corresponding colour, both of which should be unique, and we note that \texttt{classId} is only used 
                internally. The colour will be used sections 
                \ref{sec:pixel_label} and \ref{sec:training_tools} to indicate its corresponding class in a pixel 
                labelling, where the pixel labelling is either input in a ``ground truth'' image or output by our system.                

                \begin{figure}[H]
                    \centering
                    \includegraphics[scale=0.5]{ClassLabel_Forest_UML}
                    \caption{\texttt{ClassLabel} in figure \ref{fig:forest_uml} a bit closer.}
                    \label{fig:class_label_uml}
                \end{figure}

                \begin{table}[H]
                    \begin{tabularx}{\textwidth}{l|X}
                        \textbf{Function Name} & \textbf{Function Description} \\
                        \hline

                        \texttt{loadClassList} & 
                            A \texttt{static} function that returns a value of type \texttt{List<ClassLabel>} given a 
                            filename specifying ``class colour map'' as in appendix \ref{app:file_formats}, of classes 
                            specified in the file. In this function we make sure that we preserve the uniqueness of 
                            class names their associated colours. \\
                        \hline

                        \texttt{computeColourToClassMap} & 
                            Returns a value of type \texttt{Map<Integer, ClassLabel>}, a mapping from a class colours to 
                            their corresponding \texttt{ClassLabel} objects. \\ 
                        \hline

                        \texttt{computeNameToClassMap} & 
                            Returns a value of type \texttt{Map<String, ClassLabel>}, a mapping from a class names to 
                            their corresponding \texttt{ClassLabel} objects. \\
                        \hline

                        \texttt{equals} &
                            An override of \texttt{Object}'s equality function. This is needed as we will frequently 
                            use \texttt{ClassLabel} as the key to a map structure, and might not use the same instance 
                            as a key. The function checks that the class name's, class colour's and class id's are 
                            identical. \\
                        \hline

                        \texttt{hashCode} &
                            Similarly to \texttt{equals} this is overridden for a correct implementation when 
                            \texttt{ClassLabel} is used as the key in a (hash) map structure.

                    \end{tabularx}
                    \caption{Important methods implemented in the \texttt{ClassLabel} class.}
                    \label{tab:ClassLabel}
                \end{table}




            \subsubsection{Instances}
                The \texttt{Instance} class is used to specify a feature vector, or some instance of our problem. We 
                define an interface to represent the base structure of an instance. 

                \begin{figure}[H]
                    \centering
                    \includegraphics[scale=0.5]{Instance_Forest_UML}
                    \caption{\texttt{Instance} in figure \ref{fig:forest_uml} a bit closer.}
                    \label{fig:instance_uml}
                \end{figure}

                A feature vector will have some finite number of `features' that we use, the \texttt{getDimension} 
                function should return the number of features (i.e. the dimension) in the instance. The functions 
                \texttt{getNormalisationReference} and \texttt{normalise} are used for normalisation of instances. 
                During classification we may want to normalise so that small changes in data with low variance have 
                the same weighting as larger changes in data with low variance. This is explored more in section 
                \ref{sec:effect_of_normalisation}. Intuitively we define some sort of `power' value for each instance 
                and use normalisation so that all \texttt{Instance}s have the same power. It is optional to use 
                normalisation in the learning (and therefore classification) algorithms in sections \ref{sec:training} 
                and \ref{sec:classification}. 

                We also define \texttt{NDRealVector}, a concrete implementation of \texttt{Instance} which can also 
                be seen in figure \ref{fig:instance_uml}. We use \texttt{NDRealVector} to represent spectra in spectral 
                images, that is, values in $\bb{R}^N$. We can simply think of this as 
                a wrapper for a list of values. We also store the power of the spectrum also, and use this as our 
                ``normalisation reference'' value, and we define the \textit{power} of a spectrum or vector to be sum 
                of squared values in the list.

                \begin{table}[H]
                    \begin{tabularx}{\textwidth}{l|X}
                        \textbf{Function Name} & \textbf{Function Description} \\
                        \hline

                        \texttt{getDimension} & 
                            This returns $n$ if there are $n$ values in \texttt{vector}, our list of doubles.  \\ 
                        \hline

                        \texttt{getPower} & 
                            This returns the power of the spectrum of values. If we consider the list of values 
                            to be $\vc{v}$ then we return $||\vc{v}||^2$. \\ 
                        \hline

                        \texttt{getNormalisationReference} & 
                            This function returns the power of the spectrum. \\
                        \hline

                        \texttt{normalise} & 
                            Normalisation of vectors can be implemented by scalar division of the vector with the power. 
                            I.e. we divide all values in the spectrum by the power. \\ 
                        \hline

                        \texttt{toString} & 
                            We override \texttt{Object}'s \texttt{toString} method. This is used later in section 
                            \ref{sec:training_tools} when we wish to print a training sequence to a text file. It 
                            simply returns a comma separated list of values. 

                    \end{tabularx}
                    \caption{Important methods implemented in the \texttt{NDRealVector} class.}
                    \label{tab:NDRealVector}
                \end{table}



            \subsubsection{Probability Distributions} \label{sec:prob_dist}
                As discussed in section \ref{sec:rand_forest} probability distributions are associated with each node, 
                and is the effective output from the classification algorithm. So any implementation of Random Forests 
                needs to have some way to represent probability distributions over possible classifications. Although 
                we can represent a probability distribution as \texttt{Map<ClassLabel, Double>}, we choose to 
                encapsulate this in it's own class \texttt{ProbabilityDistribution} so that we can perform validation
                on the properties of a probability distribution, that is each probability is a value in $[0,1]$ and 
                the probabilities sum to a $1$ and we also cache some frequently used values such as \texttt{entropy}. 

                The \textit{entropy} of a probability distribution $p:\Omega\rightarrow[0,1]$ is defined to be

                \begin{align}
                  H(p) = \sum\limits_{x\in\Omega} -p(x) \log_2(p(x))
                  \label{eq:prob_entropy}
                \end{align}

                which is consistent with the definition of entropy given in equation \ref{eq:training_seq_entropy}.

                \begin{figure}[H]
                    \centering
                    \includegraphics[scale=0.5]{ProbabilityDistribution_Forest_UML}
                    \caption{\texttt{ProbabilityDistribution} in figure \ref{fig:forest_uml} a bit closer.}
                    \label{fig:prob_dist_uml}
                \end{figure}

                We make the class immutable so that we cannot accidentally modify the distribution into something 
                invalid. We make it immutable by making each variable \texttt{final} and removing any setter functions. 
                We however also make use of the \texttt{unmodifiableMap} function in the \texttt{Java.Collections} 
                package, when setting the \texttt{probabilities} variable. This gives a reference to a \texttt{Map} 
                where no entries can be added, nor removed, which prevents someone from getting a reference to the map
                \texttt{probabilities} and altering it.

                Also in the implementation of the class constructor we allow for a small error in the sum, this is 
                because we will often have small errors from rounding in floating point numbers. So the checks that 
                are made in the constructor of \texttt{ProababilityDistribution} are:
                \begin{itemize}
                    \item 
                        each value is in the range $[0,1]$;
                    \item 
                        the sum of values is in the range $[1-\epsilon,1+\epsilon]$, for some $0 < \epsilon \ll 1$.
                \end{itemize}

                Another implementation problem is found in the computation of entropy, as $-y\log_2(y)$ isn't defined for 
                the value of $y=0$. Mathematically we solve this problem by setting

                \begin{align}
                    -y\log_2(y)|_{y=0} & \defeq \lim_{y\to 0} -y \log_2(u) \\
                        & = 0.
                \end{align}

                Now when we compute entropy according to equation \ref{eq:prob_entropy} we try to sum with $p(x)=0$ for 
                some $x$. In this case we would accidentally set the sum to a \texttt{NaN} value

                \begin{align}
                  \texttt{sum} &= \texttt{sum} + 0.0 * \log(0.0) \\
                    &= \texttt{sum} + 0.0 * -\infty \\
                    &= \texttt{sum} + \texttt{NaN}\\
                    &= \texttt{NaN},
                \end{align}

                where $\infty$ is the floating point representation of infinity, and we have 

                \begin{align}
                  \log(0.0) &= -\infty, \\
                  0.0 * \infty &= \texttt{NaN}.  
                \end{align}

                in accordance with the IEEE 754 standard \cite{1985--ieee754}. We hence need to skip over updating the 
                accumulating \texttt{sum} variable when $p(x) = 0$ otherwise we will accidentally set \texttt{sum} to a 
                \texttt{NaN} value.

                \begin{lstlisting}[caption={Part of the \texttt{ProbabilityDirstribution} constructor, where we set $\epsilon = 2^{-10}$.}]
public class ProbabilityDistribution implements Serializable {

  ...

  public ProbabilityDistribution(Map<ClassLabel, Double> distribution, int noClasses) 
      throws MalformedProbabilityDistributionException {
    
    ...
    
    // Check that the sum is 1, allowing for a small floating point error
    double eps = 1e-10;
    if (sum < 1.0-eps || 1.0+eps < sum) {
      throw new MalformedProbabilityDistributionException;
    }
    
    // Assign the values (so they are immutable)
    this.probabilities = Collections.unmodifiableMap(distribution);
    this.mostProbableClass = computeMostProbableClass();
    this.entropy = computeEntropy();
  }

  ...
}
                \end{lstlisting}                


                \begin{table}[H]
                    \begin{tabularx}{\textwidth}{l|X}
                        \textbf{Function Name} & \textbf{Function Description} \\
                        \hline

                        \texttt{computeMostPorbableClass} & 
                            Returns the \texttt{ClassLabel} with the highest probability in the distribution. This 
                            function is labelled private and is only used in the constructor. \\ 
                        \hline

                        \texttt{mostProbableClass} & 
                            Getter for the \texttt{mostProbableClass} value. \\ 
                        \hline

                        \texttt{computeEntropy} & 
                            Computes the entropy and returns the value. This function is labelled private and is only 
                            used in the constructor. \\ 
                        \hline

                        \texttt{entropy} & 
                            Getter for the \texttt{entropy} value. \\ 
                        \hline

                        \texttt{equals} & 
                            Override of \texttt{Object}'s function \texttt{equals}, returns true if the variable 
                            \texttt{probabilities} are equal in the two objects. We want this when comparing 
                            probability distributions rather than referential equality. \\ 
                        \hline

                        \texttt{hashCode} & 
                            Override of \texttt{Object}'s function \texttt{equals}, as we have overridden the 
                            \texttt{equals} function. \\ 

                    \end{tabularx}
                    \caption{Important methods implemented in the \texttt{ProbabilityDistribution} class.}
                    \label{tab:ProbabilityDistribution}
                \end{table}



            \subsubsection{Training Sequences}
                The \texttt{TrainingSample} object is simply a pairing between a \texttt{ClassLabel} and an 
                \texttt{Instance}. There is no other functions in this class other than the getters, setters and 
                constructor. 

                A \texttt{TrainingSequence} consists of a list of \texttt{TrainingSample}s, and we keep a list of 
                \texttt{ClassLabel}s for reference. A number of convenience functions are defined in the class and 
                are listed in table \ref{tab:TrainingSequence}, which are used to compute useful values, such as
                entropy and information gain from equations \ref{eq:training_seq_entropy} and \ref{eq:information_gain} 
                respectively.

                \begin{figure}[H]
                    \centering
                    \includegraphics[scale=0.5]{TrainingSequence_Forest_UML}
                    \caption{\texttt{TrainingSequence} and \texttt{TrainingSample} in figure \ref{fig:forest_uml} a bit closer.}
                    \label{fig:training_seq_uml}
                \end{figure}

                \begin{table}[H]
                    \begin{tabularx}{\textwidth}{p{4.5cm}|X}
                        \textbf{Function Name} & \textbf{Function Description} \\
                        \hline

                        {\tt newNDRealVector\-TrainingSequence} & 
                            \texttt{newNDRealVectorTrainingSequence} is a \texttt{static} function taking two file names. 
                            One specifying 
                            a ``class colour map'' file and one specifying a training sequence. Both these files should 
                            be of the format described in appendix \ref{app:file_formats}. This function parses the 
                            files and returns a \texttt{TrainingSequence} instance generated from it, with instances 
                            of the type \texttt{NDRealVector} defined in listing \ref{lst:NDRealVector}. \\ 
                        \hline

                        \texttt{empericalDistribution} & 
                            Returns a \texttt{ProbabilityDistribution} that represents the empirical distribution of the 
                            training sequence, as defined in equation \ref{eq:empirical_distribution}. \\
                        \hline

                        \texttt{entropy} & 
                            This returns the entropy of the empirical distribution. It is essentially just a shorthand 
                            for calling \texttt{empiricalDistribution().entropy()}. \\
                        \hline

                        \texttt{join} & 
                            A \texttt{static} function that takes two \texttt{TrainingSequence}s, and joins them 
                            together. This simply appends the lists of \texttt{TrainingSample}s from two 
                            \texttt{TrainingSequence}s into a new \texttt{TrainingSequence} instance. \\
                        \hline

                        \texttt{informationGain} & 
                            A \texttt{static} function that takes two \texttt{TrainingSequence}s \texttt{ts1} and 
                            \texttt{ts2}. The function returns the information gain for splitting the training 
                            sequence \texttt{join(ts1,ts2)} into the training sequences \texttt{ts1} and \texttt{ts2} 
                            according to equation \ref{eq:information_gain}. \\
                        \hline

                        \texttt{normalise} & 
                            This parses all of the \texttt{Instance}s in the training sequence and computes an average 
                            ``normalisation reference'' (see \texttt{getNormalisationReference} in listing 
                            \ref{lst:Instance} and in table \ref{tab:NDRealVector}). It then returns a new training 
                            sequence where every sample has been normalised to this average value. \\
                        \hline

                        \texttt{saveToTextFile} & 
                            This \texttt{static} function writes out a training sequence file with the data from the 
                            given \texttt{TrainingSequence} instance, in accordance with the file format specified in 
                            appendix \ref{app:file_formats}. \\

                    \end{tabularx}
                    \caption{Member functions of the \texttt{TrainingSequence} class.}
                    \label{tab:TrainingSequence}
                \end{table}





            \subsubsection{Split Parameters} \label{sec:split_params}
                We define an abstract class \texttt{SplitParameters}, which is used as a common superclass for any 
                implementation of a split parameter, as defined in section \ref{sec:intro_decision_tree}. We 
                make this an abstract class so that can implement \texttt{Serializable} forcing
                any implementing classes to also implement this interface, which is 
                necessary to be able to save decision forests to a file in section \ref{sec:DecisionForest}. 
                We also specify that each split parameter needs to be 
                able to specify what weak learner it is for, because we only save the split parameters in 
                decision trees and we need some way to determine which \texttt{WeakLearner} subclass to use from a 
                \texttt{SplitParameter} instance. 

                We consider also a concrete implementation of \texttt{SplitParameters}, specifically 
                \texttt{OneDimensionalLinearSplitParameters} which specifies a ``dimension'' and a threshold as needed 
                for the \texttt{OneDimensionalLinearWeakLearner} in section \ref{sec:weak_learner}. 

                The \texttt{SplitParameter} classes are simple and only include getters, setters and constructors. That 
                is there are no `interesting' functions in these classes.

                \begin{figure}[H]
                    \centering
                    \includegraphics[scale=0.5]{SplitParam_Forest_UML}
                    \caption{\texttt{SplitParamter} and \texttt{OneDimensionalLinearSplitParameter} in figure 
                    \ref{fig:forest_uml} a bit closer.}
                    \label{fig:split_param_uml}
                \end{figure}




            \subsubsection{Weak Learners} \label{sec:weak_learner}
                We implement \texttt{WeakLearner} as an abstract class, because we provide a partial 
                implementation. Any common code between subclasses can be put directly into the \texttt{WeakLearner} 
                abstract class, such as helper functions for generation of randomised parameters. The function of each 
                abstract method is explained in table \ref{tab:OneDimensionalLinearWeakLearner}. The function 
                \texttt{uniformRandomDoubleInRange} is a simple convenience method that is used to generate a value 
                uniformly in some range \texttt{[lowerBound,highBound]}, 
                and uses Java's implementation of generating a uniform random variable in the range \texttt{[0,1]}. To 
                identify the type of each weak learner with some tag we define an \texttt{enum} type in listing 
                \ref{lst:WeakLearnerType}, seen in figure \ref{weak_learner_type_uml}. 

                \begin{figure}[H]
                    \centering
                    \includegraphics[scale=0.5]{WeakLearnerType_Forest_UML}
                    \caption{The \texttt{WeakLearnerType} enum in figure \ref{fig:forest_uml} a bit closer.}
                    \label{fig:weak_learner_type_uml}
                \end{figure}

                We also consider a concrete implementation for an example, using the \texttt{OneDimensionalLinearWeakLearner} 
                class. This is the most simple weak learner function that we can decide and has the simplest parameters, 
                when we are considering \texttt{NDRealVector} instances. We note that subclasses of \texttt{WeakLearner}s are
                specific to particular subclasses of \texttt{Instance}. 
                The \texttt{OneDimensionalLinearWeakLearner} simply picks one value in the feature vector/instance, compares 
                it to some threshold and then makes a decision based on this. For example consider (mathematically) an 
                instance $\vc{v} \in \bb{R}^n$, in this case our split parameters are $\vc{\theta} = (i,\tau) \in 
                \bb{Z}_n \times \bb{R}$, and the split function/weak learner is

                \begin{align}
                  f(\vc{v}; \vc{\theta}) = f(\vc{v}; (i,\tau)) = \begin{cases}
                    1 & \text{ if } v_i \geq \tau \\
                    0 & \text{ otherwise.}
                  \end{cases}
                  \label{eq:split_equation}
                \end{align}

                \begin{figure}[H]
                    \centering
                    \includegraphics[scale=0.5]{WeakLearner_Forest_UML}
                    \caption{\texttt{WeakLearner} and \texttt{OneDimensionalLinearWeakLearner} in figure \ref{fig:forest_uml} a bit closer.}
                    \label{fig:weak_learner_uml}
                \end{figure}

                These split parameters for the \texttt{OneDimensionalLinearWeakLearner} class are represented by the 
                \texttt{OneDimensionalLinearSplitParameters} class as described in section \ref{sec:split_params}. This 
                is an example that each subclass of \texttt{WeakLearner} should have a corresponding subclass of 
                \texttt{SplitParameters} which it uses to represent it's split parameters. And we note that 
                \texttt{OneDimensionalLinearSplitParameters} is implemented as a nested class within 
                \texttt{OneDimensionalLinearWeakLearner}, to make this relationship obvious.


                The \texttt{split} function is of particular interest in listing \ref{lst:OneDimensionalLinearWeakLearner} as 
                it implements equation \ref{eq:split_equation}. 

                \begin{table}[H]
                    \begin{tabularx}{\textwidth}{l|X}
                        \textbf{Function Name} & \textbf{Function Description} \\
                        \hline

                        \texttt{getWeakLearnerType} & 
                            A function that is used to identify what type of weak learner an instance is. This is 
                            useful when a subclass is cast to \texttt{WeakLearner} and we want to cast it back to 
                            the subclass without causing an exception. \\ 
                        \hline

                        \texttt{split} & 
                            This function takes a split parameter and an instance, it is the concrete implementation 
                            of the split function. It returns an enum of type \texttt{Direction}, which specifies if 
                            we should traverse to the left or right child of a decision tree node. \\ 
                        \hline

                        \texttt{giveHint} & 
                            In this function we are passed the training sequence prior to training a decision tree. 
                            It is used as a chance to look at the data to give a hint to what split parameters we 
                            might want to generate, or more specifically not generate. \\ 
                        \hline

                        \texttt{generateRandomSplitParameters} & 
                            This function provides a routine for generating a random split parameter to be tried in the 
                            split function during training a decision tree. \\ 

                    \end{tabularx}
                    \caption{Member functions of the \texttt{OneDimensionalLinearWeakLearner} class.}
                    \label{tab:OneDimensionalLinearWeakLearner}
                \end{table}

                The \texttt{OneDimensionalLinearWeakLearner} makes use of the \texttt{giveHint} function by iterating through 
                all \texttt{NDRealVector} instances in the training sequence and recording a minimum and maximum value 
                for each dimension in the lists \texttt{minimumValues} and \texttt{maximumValues}. This therefore 
                allows us to avoid picking particularly bad \texttt{OneDimensionalLinearSplitParameters}, 
                as for any of the features/dimensions in the \texttt{NDRealVector} 
                that are chosen, if we pick a threshold value outside the range between the minimum and maximum we know that 
                the split function will split the training sequence with zero information gain. Using the notation from 
                equations \ref{eq:left_split} and \ref{eq:right_split} it means that one of $L(\vc{s},\vc{\theta})$ or 
                $R(\vc{s},\vc{\theta})$ will be empty, leading to an information gain of zero ($I(\vc{s},\vc{\theta}) = 0$ 
                in equation \ref{eq:information_gain}).

                \begin{figure}[H]
                  \centering
                  \includegraphics[scale=0.25]{one_dimensional_weak_learner_optimisation.jpeg}
                  \caption{Knowing the minimum and maximum values in each dimension allows us to make an informed 
                  choice on split parameters to pick, which in this case is the green zone of values.}
                \end{figure}




            \subsubsection{Decision Tree Node}
                Finally we move onto defining our decision tree, implement the \texttt{TreeNode} class nested in 
                \texttt{DecisionForest} to show that their behaviour is tightly coupled. Each node has a reference 
                to a left and right child node, which are set to \texttt{null} in a leaf node. Every node caches a 
                probability distribution, even if it is a decision node, and the 
                probability distribution that is cached is the empirical distribution of the training sequence that is 
                passed to it. Each decision node has a reference to a \texttt{SplitParameters} instance, which it 
                uses with a weak learner to make decisions for tree traversal as in section 
                \ref{sec:intro_decision_trees}. Finally we cache the information gain, computed according to the 
                equation \ref{eq:information_gain} during the training algorithm.

                \begin{figure}[H]
                    \centering
                    \includegraphics[scale=0.5]{Tree_Forest_UML}
                    \caption{\texttt{TreeNode} and \texttt{DecisionForest} in figure \ref{fig:forest_uml} a bit closer.}
                    \label{fig:weak_learner_uml}
                \end{figure}

                We see in listing \ref{lst:TreeNode} the function \texttt{compact}. This is an optimisation 
                used to eliminate unnecessary nodes from the tree after its construction. The idea is that if we are at 
                some decision node and it has two children are leaf nodes with identical distributions, then we 
                can replace this decision node by a single leaf node with the same distribution. The implementation 
                simply performs \texttt{compact} recursively on the left and right children of a decision node first, 
                and \texttt{compact} does nothing in the base case (a leaf node), after we try to compact at the 
                given node. Essentially, compaction is performed from the bottom of the tree upwards.

                We also note that the \texttt{TreeNode} class implements the \texttt{Serializable} interface, 
                which is again necessary to be able to use \texttt{ObjectOutputStream} in section 
                \ref{sec:DecisionForest}, when saving forests to a file.

                \begin{figure}[H]
                  \centering
                  \includegraphics[scale=0.35]{tree_compaction.jpeg}
                  \caption{An example of how \texttt{compact} could be used to reduce the size of a decision tree.}
                \end{figure}

                \begin{lstlisting}[caption={The \texttt{TreeNode} declaration, found as a static class within the 
                \texttt{DecisionForest} class.},label={lst:TreeNode}]
void compact() throws MalformedForestException {
  if (this.isLeafNode()) {
    return;
  }
  
  this.leftChild.compact();
  this.rightChild.compact();

  if (!this.isLeafNode() && 
      leftChild.isLeafNode() &&
      rightChild.isLeafNode() &&
      probabilityDistribution.equals(leftChild.probabilityDistribution) &&
      probabilityDistribution.equals(rightChild.probabilityDistribution)) {
    
    // Make this node a leaf node
    leftChild = null;
    rightChild = null;
  }
}
                \end{lstlisting}

                \begin{table}[H]
                    \begin{tabularx}{\textwidth}{l|X}
                        \textbf{Function Name} & \textbf{Function Description} \\
                        \hline

                        \texttt{isLeafNode} & 
                            Checks if the \texttt{leftChild} and \texttt{rightChild} have a value of \texttt{null}. If 
                            they are then the node is a leaf and this functions returns a true value. If only one of the 
                            nodes has a value of \texttt{null} then the tree is invalid and an exception is raised. 
                            Otherwise it returns a false value. \\ 
                        \hline

                        \texttt{compact} & 
                            As described above, this compacts the tree to an equivalent tree, replacing any unnecessary 
                            decision nodes with a leaf node. \\ 
                        \hline

                        \texttt{traverseTree} &
                            A method that given an instance of a \texttt{WeakLearner} and an \texttt{Instance} will 
                            traverse the tree and return a probability distribution for that instance. As described in 
                            section \ref{sec:intro_decision_trees} and visualised in figure 
                            \ref{fig:decision_tree_classify}. The algorithm this method implements is covered in more 
                            detail in section \ref{sec:classificiation}.

                    \end{tabularx}
                    \caption{Member functions of the \texttt{TreeNode} class.}
                    \label{tab:TreeNode}
                \end{table}




            \subsubsection{Decision Forest} \label{sec:DecisionForest}
                We finally define our \texttt{DecisionForest} class. It is basically a set of \texttt{TreeNode}s, 
                however we also include additional metadata, which can be seen in figure \ref{fig:weak_learner_uml}. 
                We have made sure that \texttt{TreeNode}, \texttt{WeakLearner}, \texttt{WeakLearnerType}, 
                \texttt{SplitParameters} and \texttt{ClassLabel} (which are all composite in the forest data structure) 
                implement the interface \texttt{Serializable} so that a concise ``save to file'' method can be 
                implemented. We simply use an instance of \texttt{ObjectOutputStream} in the \texttt{Java.Io} package 
                to write the object to a binary file in a single line ``\texttt{out.writeObject(this);}''.

                \texttt{dataDimension} keeps track of the dimension of data this forest classifies, the 
                \texttt{weakLearnerType} specifies what weak learner our forest is related to, and implicitly the type 
                of data that it classifies. It keeps a reference to all the possible classification labels in 
                \texttt{classes} and it uses \texttt{normalisedClassification} and \texttt{normalisationReference} to 
                perform normalisation during classification if we specified that normalised values should be used during 
                training.

                \begin{table}[H]
                    \begin{tabularx}{\textwidth}{l|X}
                        \textbf{Function Name} & \textbf{Function Description} \\
                        \hline

                        \texttt{DecisionForest} & 
                            We include the constructor here because we provide two. We have a \texttt{default} access 
                            default constructor, which is used to construct forests in the training algorithm. The 
                            other allows the forest to be loaded from a file using an \texttt{ObjectInputStream} from 
                            the \texttt{Java.Io} package. \\ 
                        \hline

                        \texttt{saveToFrstFile} & 
                            This function saves a forest to a persistent ``.frst'' file. It's implementation can be
                            found in listing \ref{lst:DecisionForest}. \\ 
                        \hline

                        \texttt{classify} & 
                            This takes an \texttt{Instance} and returns a probability distribution for it, according 
                            to the forest. The algorithm implements is discussed in more detail in section 
                            \ref{sec:classification}. \\ 

                    \end{tabularx}
                    \caption{Member functions of the \texttt{TreeNode} class.}
                    \label{tab:DecisionForest}
                \end{table}






      \subsection{Training} \label{sec:training}
          \begin{framed}
              \begin{itemize}
                  \item Walk through algorithm, with a listing(s)
                  \item Discuss information gain cutoff
                  \item Discus tradeoffs between breadth and depth first training
                  \item Abstraction of split parameter generation into the weak learner (etc.) allows for a SINGLE learning routine to be implemented for all WeakLearners.
                  \item (Still to implement). Parallellisation of training trees. Discuss that much more could be done.
                  \item (Still to implement). Bagging to prevent bias \& overfitting
              \end{itemize}
          \end{framed}




      \subsection{Classification} \label{sec:classification}
          \begin{framed}
              todo
          \end{framed}

    \section{Neural Networks}
        \begin{framed}
            \begin{itemize}
                \item Introduce encog
                \item Explain usage of the library and features we used
            \end{itemize}
        \end{framed}

        \subsection{Training}
            \begin{framed}
                \begin{itemize}
                    \item Outline how we wrapped up the training of the neural network
                    \item Describe roughly what it does?
                \end{itemize}
            \end{framed}

        \subsection{Classification}
            \begin{framed}
                \begin{itemize}
                    \item Outline how we wrapped up the classifier
                    \item Describe roughly what it does?
                \end{itemize}
            \end{framed}

    \section{Pixel Labelling} \label{sec:pixel_label}
        \begin{itemize}
            \item We've given the overview of two supervised machine learning techniques
            \item Assume for now that we have appropriate training data - dealt with later
            \item We now need to use that to provide a pixel labelling
            \item Listing for the datacube, and listing that implements the pixel labelling
            \item Assuming we have appropriate training data, and non-noisy images we are now done! Unfortunately not the case.
        \end{itemize}

    \section{Training Tools} \label{sec:training_tools}
        \begin{itemize}
            \item Take an image with a manually created ground truth pixel labelling
            \item Take these and convert into a training sequence, and output to a text file
            \item The append option -> we can pass in a perameter and an existing training sequence and append training sequences
            % \item Adding uniform data between certain points, at a given density - and why this might be useful. Novel idea of generation of data using given data and some linear model
        \end{itemize}

    \section{De-noising} \label{sec:de-noising}
        \begin{itemize}
            \item Explain total variation
            \item Minimising gradients, can think of this as removing high frequency components
            \item Listing for overview of 
        \end{itemize}

    %\section{Using the GPU using OpenCL}

    %\section{Tree analysis and principle component analysis}

    % -mention about a typical optimisation is over ||w|| + sum of errors squared -> find a paper that explains this idea and reference it!!
    % -that assumes that the features are lowely/not correlated
    % in this case THEY ARE, so we can use this as a tool and we definitely DONT want to optimise over the above!

    \section{Application on example data sets}
        \begin{itemize}
            \item We've build a system, but we need to actually apply it to some problems!
        \end{itemize} 

        \subsection{Siri's data}
            \begin{itemize}
                \item TODO: Come up with a better title for this subsection
                \item Explanation of how produced training data
                \item Example pixel labellings
            \end{itemize}

        \subsection{Teng's data}
            \begin{itemize}
                \item TODO: Come up with a better title for this subsection
                \item Explanation of how produced training data
                \item Example pixel labellings
            \end{itemize}








%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% the evaluation

\cleardoublepage
\chapter{Evaluation}
    In this section we will look at the performance of the different components of the system based on a number of 
    metrics. 

    \section{Performance measures for classifiers}
        \begin{itemize}
            \item Briefly say about the different measures as described at the end of AI II.
            \item Justify what might be the most informative here.
        \end{itemize}

    \section{Evaluation of Random Forests pixel labelling}
        \begin{itemize}
            \item Compute the performance measures for a noiseless toy image, noisy toy image and two data sets provided, and compare
        \end{itemize}


    \section{Evaluation of Neural Networks pixel labelling}
        \begin{itemize}
            \item Compute the performance measures for a noiseless toy image, noisy toy image and two data sets provided, and compare
            \item Compare with random forests - success criterion (table of results and say a bit about which did better)
        \end{itemize}

    \section{Evaluation of the Random Forests library}
        \begin{itemize}
            \item Define a standard training set, this can actually be independent of pixel labelling and similar to the Chriminsi forests paper.
        \end{itemize}

        \subsection{Training time}
        \subsection{Classification time}
        \subsection{The effect of the number of trees}
        \subsection{The effect of the depth of trees}
        \subsection{The effect of the randomness of trees}
        \subsection{The effect of normalisation} \label{sec:effect_of_normalisation}


    \section{The effect of the de-noising component}
        \begin{itemize}
            \item Use a couple images with added noise
            \item Compare the SNR for the method with respect to different noise models (does it handle any other types of noise?)
            \item Plot SNR as a function of the parameter lambda in the TVMM method
            \item Plot SNR as a function of noise power (for different types of nois)
        \end{itemize}


    % \section{Performance speed ups in OpenCL}=
    % *TODO* - compare to classification time with OpenCL in encog turned on, also compare to non openCL time.
    % Compare the classification times with openCL on/off in encog.


    % \section{Evaluation of the analysis module}
    %     \begin{itemize}
    %         \item Made for Siri's data set
    %         \item Run the analyisis
    %         \item Find the principle components
    %         \item Compare with a standard method - such as the Kovendasjdk transform
    %         \item On the reduced sets, train and compare the pixel labelling measures above
    %     \end{itemize}










%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% the conclusion


\cleardoublepage
\chapter{Conclusion}
  \section{Summary}
        \begin{itemize}
            \item Overview and summary of work undertaken - re describe system overall
            \item What was achieved, what was different
            \item What did the evaluation show?
            % \item Novel method of principle component analysis
        \end{itemize}

    \section{Further Work}
        \begin{itemize}
            \item Implement a convolutional neural network solution to allow local spatial data to influence the labelling of a pixel. This may allow the model to incorporate image de-noising and could compact two stages of the pipeline into one.
            \item Implementation of the random forests library to support GPU processing. We could use a language such as OpenCL. Parallelism could be exploited either in the random forest implementation OR/AND the inherent parallelism from performing image processing.
        \end{itemize}










%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% the bibliography

\cleardoublepage
\addcontentsline{toc}{chapter}{Bibliography}
\bibliography{refs}
%\printbibliography
\cleardoublepage










%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% the appendices
\appendix

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% description of benchmark tests

\cleardoublepage
\chapter{File formats}    \label{app:file_formats}
    Here we define the file formats that a user might be expected to input into the system, an explanation of the 
    files that are output by the system and how to interpret them.

    \section{(Example/Noisy) Spectral Image}
        ...


    \section{Example Image Labelling}
        ...
    
    \section{Label Map}
        \begin{figure}[H]
            \begin{framed}
                {\tt typewriter .txt file eg here }
            \end{framed}
        \end{figure}

    \section{Training Sequence}
        \begin{figure}[H]
            \begin{framed}
                {\tt typewriter .txt file eg here }
            \end{framed}
        \end{figure}

    \section{Output Files}
        ...









%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% evaluation data

\cleardoublepage
\chapter{A brief explanation of the method of conjugate gradients}
    TODO











%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% project proposal

\cleardoublepage
\chapter{Project Proposal}

\input{proposal}










%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% the glossary

\cleardoublepage
\chapter{Glossary}
\glsaddall
\printglossaries

\end{document}
